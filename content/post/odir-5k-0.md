---
author: "momo"
date: 2022-07-19
title: "odir-5k"
categories: [
    "论文笔记",
]
---

## ODIR-5k

原比赛：https://odir2019.grand-challenge.org/

数据集构成：这是一个由5,000名病人组成的结构化眼科数据集。提供了**八种眼病类别的多标签图像级注释，包括糖尿病、青光眼、白内障、老年黄斑变性（AMD）、高血压、近视、正常和其他疾病。每个病人可能包含一个或多个疾病标签。**模型需要开发自动眼部疾病分类的方法。以左右眼的彩色眼底图像作为输入（可以使用其他提供的信息，如患者年龄、性别），目标是将患者分为八类。

|          | 训练集 | 测试集 |
| -------- | ------ | ------ |
| 患者数量 | 3500   | 500    |
| 样本数量 | 7000   | 1000   |

| 正常 | 糖尿病 | 青光眼 | 白内障 | AMD  | 高血压 | 近视 | 其他疾病/异常 |
| ---- | ------ | ------ | ------ | ---- | ------ | ---- | ------------- |
| 1140 | 1128   | 215    | 212    | 164  | 103    | 174  | 979           |

评估指标：参与者必须提交所有测试数据的八类分类结果。对于每个类别，分类概率（值从 0.0 到 1.0）表示被诊断为相应类别的患者的风险。 提交的内容根据三个指标进行评分：[kappa score ](https://en.wikipedia.org/wiki/Cohen's_kappa)、  [F-1 socre](https://en.wikipedia.org/wiki/F1_score) 和 [AUC value](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve)。 阈值为 0.5。

![](https://halfbit.oss-cn-hangzhou.aliyuncs.com/12791656585757_.pic.jpg)

## 下载地址

1.原比赛网址链接失效，kaggle下载（附带kaggle预处理版本的数据集）：https://www.kaggle.com/datasets/andrewmvd/ocular-disease-recognition-odir5k/code （预处理图片：只有训练图片，一共6392条记录，512大小，但是csv文件里面，对多标签图片只打了一个标签）

2.https://www.heywhale.com/mw/dataset/5e95e3ede7ec38002d0351f6

3.https://www.icode9.com/content-2-869077.html

已下载至论文积累/odir-5k文件夹。

## 训练标注

![](https://halfbit.oss-cn-hangzhou.aliyuncs.com/2022-07-215.03.28.png)

3500个病人。包含患者年龄，患者性别信息。id为288的患者对应的左右眼图片分别是288_left、288_right。每个患者都有2张图片，没有缺失。描述关键词：中度非增殖性视网膜病变、轻度非增殖性视网膜病变、严重非增殖性视网膜病变、正常眼底图、视网膜激光手术后，中度非增殖性视网膜病变、视网膜激光手术后，糖尿病性视网膜病变、视网膜内微血管异常等，但是这个描述性关键词和八个病变的标签没有直接联系（左右眼不同的描述组合，可能标签是一样的）。有多标签、有单标签数据。每个标签的数量，是不均衡的（单标签里面，N和D的比较多；多标签里面，G和O就比较多）。一位患者对应两张图片，对应一组标签。两个norm，最后的标签是norm；一个norm，那么对病变图片的标签就是这个患者的标签；如果两只眼睛都是病变，那其实每张图片的标签是不确定的·。

## 训练图片

![](https://halfbit.oss-cn-hangzhou.aliyuncs.com/2022-07-215.10.28.png)

数据分析 https://www.kaggle.com/code/ulafiliz/ocular-disease-detection-data-analysis ：

1.对于给定数据集中的女性和男性的比较，男性的糖尿病视网膜病变略高，尽管男性眼底样本被诊断为 "正常 "的较多。
2.近视更可能发生在年轻男性身上。
3.大多数疾病的年龄分布是正常的。
4.女性比男性更有可能在早期被诊断出青光眼。

![](https://halfbit.oss-cn-hangzhou.aliyuncs.com/2022-07-215.44.05.png)

## ODIR-5k有关的论文

#### CLASSIFICATION OF OCULAR DISEASES EMPLOYING ATTENTION-BASED UNILATERAL AND BILATERAL FEATURE WEIGHTING AND FUSION

2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)

基于注意力的融合网络（AUBNet），将患者自动分类到相应的疾病类别中。具体来说，AUBNet由一个特征提取模块（FEM）、一个特征融合模块（FFM）和一个分类模块（CFM）组成。FEM从患者的双侧眼底照片中独立提取两个特征向量。通过FFM，进行两级特征加权和融合，以准备双眼的特征表示。最后，多标签分类由CFM进行。

这篇文章重在阐述它的FFM即特征融合这一部分，出发点是给左右眼的特征向量不同的权重，主要是在说明这个权重的计算是怎么计算的。这里的注意力就是用线性层（全连接层（权重、偏置）实现）。最终得到的向量，经过2个全连接层做分类。2048-512-8。最后用的loss函数是BCE二元交叉熵损失函数。

三个指标：kappa得分，f1得分，auc。最后加一个加权总分（平均）。

作者将数据集分成3部分，1167、1167和1166名患者。三折实验。

特征融合的部分，做了对比实验。相加、通道连接、乘法和提出的进行对比。特征提取器分别使用resnet18、34、50、101。训练的时候，图像resize成512*512；训练过程中，random crop成448大小。测试时候用的是center crop。用的SGD优化器。学习率策略：The poly learning rate decay policy is used with an initial learning rate of 0.007, power of 0.9, and epoch of 50.

![](https://halfbit.oss-cn-hangzhou.aliyuncs.com/2022-07-204.12.22.png)

第二个消融实验。只有一次注意力融合：把中间紫色部分（只做一阶融合）的向量，用于分类。接CFM。只有两次的融合：用计算出的beta权重和最前面的红灰向量做加权相加。两个融合结合：本文最早提出的方法。发现最后一种效果是最好的。

为了验证注意力的权重是不是正确，分类标签是在病人层面上给出的，个别眼睛有诊断关键词。根据这些关键词，我们只能区分眼睛是否受到影响，而不能确定具体的眼部疾病。为了简化结果的解释，我们将200个只有一只眼睛受影响的病人的学习权重可视化。总的来说，患病的眼睛得到的权重更大。而平均权重也存在明显的差异——患病的眼睛权重比正常眼睛的权重要高。

![](https://halfbit.oss-cn-hangzhou.aliyuncs.com/2022-07-204.38.34.png)

作者对CFM这个模块没有详细说，sigmoid+bceloss。https://zhuanlan.zhihu.com/p/500347063

论文无代码。

#### CCT-Net: Category-Invariant Cross-Domain Transfer for Medical Single-to-Multiple Disease Diagnosis

2021 ICCV

类别不变的领域迁移。单疾病诊断-多疾病诊断

1.将糖尿病视网膜病变扩展到识别多种眼部疾病。2.将胶质瘤识别扩展到其他脑肿瘤的诊断。

从带有精细注释的单一疾病诊断中学习到的知识，可以被转移到改善没有注释的多种相关疾病的诊断中。源领域需要有像素级别的标注，并且是单一标签。然后目标域是没有标注的，多标签。只知道图像级的疾病类别。

![](https://halfbit.oss-cn-hangzhou.aliyuncs.com/2022-07-205.53.11.png)

本文重点：1. 设计了一个特定领域的任务学习模块，以学习领域的变量特征，同时保留两个领域之间的疾病差异。提出了一种CWP全局汇集方法，以获得比其他全局汇集操作更好的类激活图（CAMs）。2. 以不同类别的粗略热图为条件，我们提出了CIFR区块来构建CCTNet，以定位相应疾病的更多鉴别性区域。类别不变的特征使其能够从源域转移到目标域。此外，这种重新定义的特征也有助于提高最终分类的性能。3.实验评估是在两个流行的医学成像任务中进行的。首先，我们将眼底图像的DR诊断扩展到多种眼部疾病的诊断，如青光眼和高血压。第二，利用脑部MRI扫描的胶质瘤分割来改善其他肿瘤的分割和分类性能，如脑膜瘤和垂体瘤。实验结果证明了我们方法的有效性。

本文任务：（从overall开始看），作者的目标是不光光提升目标领域对于图像类别的诊断，还有疾病的定位。

![](https://halfbit.oss-cn-hangzhou.aliyuncs.com/2022-07-206.07.10.png)

DSE （Domain specific encoder）：

两个领域共享卷积（Conv）参数，以约束编码器学习领域不变的表征。领域特定批量归一化（BN）来加强对领域不变特征的学习。为了构建DSE，我们采用DenseNet-121[24]作为骨干。在每个Conv层之后，不同领域的BN层被分别学习。

除了获得不同疾病的初步分类结果外，DSE还能计算不同类别的粗略热图。在图像分类任务中，仅使用图像级标签的弱监督方法[8, 50]通常采用类激活图（CAM）[55]来计算定位每个类别的粗略热图。一个特定类别的CAM表明网络用来识别该类别的鉴别性图像区域，并可用于解释网络做出的预测决定。这种定位能力是由基本全局平均/最大集合（GAP/GMP）层实现的。虽然GAP和GMP已被广泛使用，但它们是不可训练的，而且CAM可能无法定位最具有鉴别力的区域。Log-Sum-Exp（LSE）池化[43]引入了一个超参数γ，作为最大池化和平均池化之间的一个可调节选项。然而，LSE池仍然是不可优化的，并且存在过流或欠流问题。

在这项工作中，我们提出了一种可训练的CWP全局池化方法，以更好的定位性能增强原始CAM。源域和目标域的任务学习。尽管最初的基于CWP的CAM已经摒弃了大面积的不相关区域来区分正确的疾病，但其粗略的热图仍然包含一些来自噪声或错误分类的干扰信息。在这项工作中，我们的目标是将这些粗略的热图重新细化为细化的像素级预测，以划定更多的鉴别区域，并将这种重新细化的能力从源域转移到目标域。请注意，CAM可能不是最好的弱监督定位方法[8]，但这不是这项工作的主要关切。我们将在今后的工作中专门研究这一点，以增强CCT-Net的功能。

CCT-Net堆叠了多组CIFR块。

分类结果如下（高于上文）

论文无代码。

![](https://halfbit.oss-cn-hangzhou.aliyuncs.com/2022-07-219.57.25.png)



#### Discriminative Kernel Convolution Network for Multi-Label Ophthalmic Disease Detection on Imbalanced Fundus Image Dataset

arxiv在投 用于不平衡眼底图像数据集的多标签眼科疾病检测的判别性核卷积网络

这项工作提出了一个鉴别性核卷积网络（DKCNet），它在不增加额外计算成本的情况下探索了鉴别性区域特征。DKCNet由一个注意块和一个SE块组成。注意区块从主干网络中获取特征，并生成鉴别性特征的注意图。（SE）区块采用鉴别性特征图并改善通道的相互依赖性。骨干网络被用来获取全局特征图。任何基于CNN的深度学习模型在ImageNet数据集上的预训练都可以作为骨干网络，从模型的最后一层提取特征图，其中包含眼底图像的高级语义特征。这些特征图为F （H×W×C），其中W、H和C为宽度、高度和特征图的通道数。主干网络的输出被送入注意力模块，该模块学习更多的区域性特征来区分病变部位。 A dropout layer follows this to reduce overﬁtting with a drop rate of 0.3. 这些鉴别性特征由SE块处理，它动态地重新校准通道。最后，一个全局平均集合层和一个全连接层通过预测类标签概率来进行疾病分类。

**动机：标准的卷积有一个固定的内核，通过在特征图上滑动来捕捉上下文信息。在这种情况下，一组相似的像素的特征可能在其他区域有不同的表现，导致类内不一致。人们普遍认为，通过使用不同的接受场大小生成多尺度特征，可以捕捉到更多的背景信息[34], [35]。扩张卷积通过改变核大小（称为扩张率）来捕捉多尺度信息。有了大的接受场尺寸，就可以捕捉到更多的语义信息。**如图所示。4，扩张卷积在拟议的模型中被用来捕捉多尺度的特征，而不增加额外的计算成本。

从主干网中提取的特征图被传递给channel shufﬂe block，对特征图进行重组，（将输入层分为 g 组，总通道数为 g × n，首先你将通道维度拆分为 (g,n) 两个维度，然后将这两个维度转置变成 (n,g) ，最后重新reshape成一个维度 g × n）并允许数据流跨越特征通道。DKCNet块接受来自channel shufﬂe block的输入。它使用2×2核进行扩张卷积运算，d取2-4。扩张卷积（DConv）之后是批量归一化（f BN）和ReLu激活（f ReLu）函数。

用R和Pi依次做元素相乘。

![](https://halfbit.oss-cn-hangzhou.aliyuncs.com/2022-07-215.27.04.png)

作者对数据集的调研：本工作中用于实验的ODIR-5K数据集是通过北京大学的一项大挑战而在线提供的。该数据集包含约5000张有组织的患者左右眼的眼底图像。这些眼底图像是用不同的眼底相机拍摄的，如Kowa、Zeiss和Cannon，具有不同的图像分辨率。疾病诊断的关键词是由眼科专家分配给这些图像的[9]。基于这些诊断关键词，疾病分类标签被分配给每对眼底图像。与其他可公开获取的数据集相比，这个视觉病理学数据集是独一无二的，因为它包含了一个病人的左眼和右眼的彩色眼底图像，在一张图像上有单一/多种异常。该数据集包含一对眼睛图像的共同目标标签。这些图像被分为八个疾病类别，正常、糖尿病、青光眼、白内障、AMD、高血压、近视和其他。患者的年龄和性别也被包括在内。“ODIR5K数据集的另一个具有挑战性的部分是，其他类图像包含与12种不同眼科疾病有关的病变”，（待确认）。在这种情况下，要学习适当的特征并不容易。另外，考虑到八个类别中每个类别的图像数量，该数据集是高度不平衡的。这些问题对分类训练模型的准确性和损失有负面影响。尽管ODIR-5K数据集更适用于现实生活中的临床情况，因为这些图像是在不同的光照条件下用不同的相机拍摄的，这对任何疾病识别模型都是一个巨大的挑战。

数据预处理：利用图像裁剪操作，使这些图像适合于模型训练。在图像裁剪操作中，通过寻找输入眼底图像中非黑色像素的起始位置来确定视野。这个位置被用来确定裁剪操作的图像掩膜边界。为了支持不同的DNN模型，图像的大小需要明确调整。因此，裁剪后的图像尺寸保持为224×224像素，因为它是大多数DNN模型普遍接受的尺寸。

**正常、糖尿病和其他类别的图像数量明显较多。**在这项工作中使用了随机采样技术来解决类的平衡问题。过度采样和欠抽样被用来创建两个不同版本的数据集，以实现拟议的CNN模型。

![](https://halfbit.oss-cn-hangzhou.aliyuncs.com/2022-07-215.25.43.png)

随机采样（过采样和欠采样）：

过度取样。生产合成图像样本的最流行的方法是为少数类别生成具有随机属性的图像。从表一中可以看出，与其他类别相比，高血压、近视、白内障、AMD和青光眼这五种类别的样本数量较少。这些类别的新样本是使用不同的数据增强策略合成的。新的样本量按公式（1）计算。M= N× (1 + k)。N代表某个小类的样本总数，M代表合成样本数。k被定义为类平衡因子（CBF），其值从1到13不等，代表扩增操作的数量。对于数据增强，使用了ﬂip、按比例（0.5、0.7、0.8和0.9）重新缩放、裁剪、旋转、对比度变化、色调、饱和度和伽玛值变化等操作。CBF也就是k在文中是指定的。	

欠采样：M=N/k，k也是指定的，按样本数量最少的定为k=1（不变），其他都随机采样，缩小样本数量。

G. 实验设置

在本研究中，通过对预训练的ResNet、InceptionV3和InceptionResNet架构的实验，选择一个骨干网络。ODIR-5K数据集被分成80%和20%，分别作为训练和验证集。对于过度采样，高血压、近视、白内障、AMD和青光眼疾病类别的CBF值分别选择为12、6、5、7、5；而正常、糖尿病和其他疾病类别的CBF值为0。同样，对于取样不足，CBF值选择为12、11、10、1，用于正常、糖尿病、其他和高血压类；2用于青光眼、白内障、AMD和近视类。表I中描述了由过量采样和欠量采样合成的样本。模型用SGD优化器进行优化。初始学习率被设定为0.0005，衰减因子为1e -6，批次大小为16。所有的模型都用BCE损失函数训练了100个epochs。所有的实验工作都在拥有16GB内存的NVIDIA T4 GPU上进行。两个实验方案已被实施，以研究DKCNet的性能。在第一种情况下，该模型通过骨干网络进行训练，对八种眼科疾病进行分类。在第二种情况下，训练是在骨干网络与DKCNet的融合上进行的。

所提议的模型的性能是用接收操作曲线下的面积（AUC）、F1分数和卡帕分数来评估的。AUC曲线是分类问题的一个性能估计器。AUC值接近于1意味着模型在分类疾病标签方面的性能更好。同样，F1分数被定义为召回率和精确度的谐波平均值，如公式（13）所示。Kappa分数是衡量实例分类结果与地面真实标签匹配程度的一个比例。它的计算方法如公式（14）所示。

实验结果：（过采样和欠采样作为对比）

![](https://halfbit.oss-cn-hangzhou.aliyuncs.com/2022-07-2111.12.52.png)



![](https://halfbit.oss-cn-hangzhou.aliyuncs.com/2022-07-2111.15.22.png)



为了检查所提出的模型对训练数据的偏倚性，它在三个公开的基准数据集上进一步测试。Messidor（糖尿病视网膜病变），G1020（青光眼），联合汕头国际眼科中心（多类）。从表四可以看出，所提出的DKCNet可以有效地预测完全未见过的眼底图像数据集上的视网膜疾病。（也是用AUC和F1）。

通过使用Grad-CAM[36]对激活图进行可视化，对结果进行定性分析。在图6中，第一列显示了包含单一疾病类别的眼底图像，病变部分用白色箭头标记。骨干网络的结果，即InceptionResnet，在第二列中被可视化了。第三列显示了通过使用DKCNet和骨干网络得到的重新定义的激活图。在第一行中，骨干网络产生的结果显示，在正常的眼睛图像中对病变部分的检测是错误的，而所提出的模型能够有效地辨别这种情况。同样，在第二行中，骨干网络未能检测到输入图像中的一些病变部分，但建议的模型能够有效地识别这些病变。第三行对应的是骨干网络和建议模型的预测结果相似的情况。在第四行中，骨干网络错误地突出了眼睛较大部分的病变部分，而提议的模型则重新确定了检测结果，并接近地面真相。

同样，多类分类的性能也可以在图7中得到体现。这里，第一列显示了有多种疾病的患者的眼底图像的输入。第二列和第三列对应的是骨干网络产生的预测类激活图，而最后两列显示的是用DKCNet获得的具有更好的视觉分类的重新定义的类激活图。在图7第一行的第四列，可以看出骨干网络未能检测到一些病变部位，而建议的方法却能很好地检测到这些部位。在第二行中，骨干网络对病变部分显示出错误的检测，而拟议的DKCNet则更准确地突出了这些部分，如图7的第四和最后一列所示。

VI. 结论和未来的工作

本工作中提出的DKCNet结构使基于CNN的模型能够在不引入额外成本的情况下学习带有注意力模块的判别性特征。这项工作的创新之处在于，该模型提高了眼科疾病分类的性能，并解决了高度不平衡的ODIR5K数据集的类平衡问题，该数据集对患者的左眼和右眼的眼底图像有多个共同的标签。DKCNet由一个注意块和一个SE块组成。注意区块从主干网络中获取特征，并生成鉴别性特征的注意图。SE区块采用鉴别性特征图，几乎在没有计算成本的情况下执行通道明智的注意力。我们用三种基于主干CNN的架构进行了实验，发现与使用InceptionResnet模型的对应方法相比，所提出的DKCNet显示出优越的性能。由于获得高质量和有标签的眼底图像的成本很高，因此计划使用生成式对抗网络来生成少数类别的样本。此外，该模型还可以被用来定位和发现病变的类型。

我的理解这篇文章应该是，最后用的sigmoid+bceloss。This work deals with a multi-class multi-label classiﬁcation problem; one or more disease labels are required as the output for each left and right eye image input. For the computation of the difference between predicted target labels and actual labels, the Binary CrossEntropy (BCE) loss function is used, which is given as……

我觉得这篇文章很好读，思路很清楚的，也总结了别人怎么去做odir-5k这个任务。主要就是1.过采样、欠采样（我觉得这里，我应该是不用做的，因为每个病的数量都差不多）2.DKCNet Block好像也没啥新鲜的捏。就酱吧。

补充：

https://blog.csdn.net/ding_programmer/article/details/104544579

https://blog.csdn.net/qq_42057562/article/details/118491412

## 其他相关论文

1.https://arxiv.org/abs/2008.09772  A Benchmark for Studying Diabetic Retinopathy: Segmentation, Grading, and Transferability -- IEEE Transactions on Medical Imaging (2020)

2.https://www.jmir.org/2021/7/e27822/ 

Islam等人[17]提出了一个基于CNN的浅层模型，从头开始训练，对ODIR-5K数据集的眼底图像进行分类。左眼和右眼的眼底图像被独立地输入到CNN模型，并相应地分配疾病标签。他们的方法使疾病分类模型不那么复杂，但他们的模型不能区分多种疾病。Wang等人[18]使用灰色和彩色直方图均衡化对眼底图像进行预处理。还使用了各种数据增强技术。预处理后的灰色和彩色图像被应用于两个并行的EfifcientNet模型，并在最后一层进行特征串联以进行最终分类。但他们在ODIR-5K数据集上只能达到73%的AUC和88%的F1分数。

Li等人[19]提出了一个密集相关网络（DCNet），使用基于转移学习的ResNet架构。空间相关模块（SCM）是该网络架构的基本构件。SCM模块确定了从彩色眼底图像中提取的特征之间的像素级致密相关关系。这些相关的特征被融合在一起，形成最终的特征图，用于对ODIR-5K数据集的眼科疾病类别进行分类，AUC为93%，F1分数为91.3%。同样，Gour和Khanna[20]为ODIR-5K数据集提出了一个预训练的双输入CNN架构。他们将左眼和右眼眼底图像同时应用于两个平行预训练的VGG-16来提取特征[21]，这些特征被连接起来以创建最终的特征图。尽管使用了VGG模型，他们还是未能超越[19]的性能。

Li等人[22]选择了VGG-16、ResNet、Inception-v4和Densenet[23]架构，对从基线模型中提取的特征进行和、乘和连接的操作。他们发现，与其他两种方法相比，对特征图的元素加和操作能产生更好的异常检测。Lin等人[24]提出了一个基于图卷积网络（GCN）的自我监督学习模型，称为MGC-Net。GCN被用来捕捉多标签眼底图像的上下文信息，而自我监督学习则用于网络的泛化。与骨干网络相比，他们的模型在ODIR数据集的眼底疾病分类中显示出性能的提升。Ou等人[25]提出了两个基于CNN的多尺度模块的注意力模型，用于多标签眼底图像分类。多尺度模块利用3×3和1×1扩张卷积器来捕捉多尺度特征。空间注意模块用于增强特征和学习全球和本地信息之间的相互依赖性。该模型在计算上被认为是有效的，但在性能上无法超越Li等人[19]。

## 组会记录

主要记在微信上；

1.有一个想法就是，单标签和多标签混合，对它们有相同的权重；但是医生在学习的时候，可能是先学习单标签的图像，会对单标签的图像有更深的印象，然后再去多标签图像中去学习。所以图片应当是分成两类的。一种方法，可以像它那样，不同权重然后融合；第二种策略，就是用单标签上提取得到的特征，领域迁移什么的。类似于这样的做法。

2.多模态融合，它用的空洞卷积，我用的是特征空间上的数学操作。
