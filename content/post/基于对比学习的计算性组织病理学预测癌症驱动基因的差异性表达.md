---
author: "momo"
date: 2022-05-09
title: "基于对比学习的计算性组织病理学预测癌症驱动基因的差异性表达"
categories: [
    "论文笔记",
    "医学图像分析",
    "arxiv",
]


---

## 我的启发

因为是比较新的刚挂上arxiv的文章，刚好也是做病理图像+对比学习无监督预训练，所以拿来参考一下思路。

## 引用

Contrastive learning-based computational histopathology predict differential expression of cancer driver genes,

2022,

https://arxiv.org/abs/2204.11994.


## 简介

这篇文章提出了一个自监督的对比学习框架HistCode，以推断出整个幻灯片图像（WSI）中的差异性基因表达。

We leveraged contrastive learning on large-scale unannotated WSIs to derive slide-level histopathological features in latent space, and then transfer it to tumor diagnosis and prediction of differentially expressed cancer driver genes. 基于对比学习的无监督预训练，得到的特征用来处理两个下游任务：肿瘤检测和基因表达。

## key messages

1、自监督对比学习被应用于大规模无标签的数字病理图像，并提取了patch级别的特征，然后通过**attention pooling**来建立幻灯片级的特征。对抗性的负面样本被生成以提出挑战，推动自我监督学习从大规模无标签的瓷砖中获取信息性的表示。

2、计算的病理特征已被证明对肿瘤诊断和不同的基因表达都有很高的预测性。有趣的是，预测准确率与倍数变化水平**（ fold-change level）**呈正相关，这表明基础分子表达模式的巨大变化将更多地反映在**表型特征**中。（没懂什么意思）

3、我们通过空间解卷积（**spatial deconvolution**）探索了模型的可解释性，并根据归一化的注意力得分（**normalized attention scores**）对每块patch进行着色。关注度高的patch的空间定位显示与经验丰富的病理学家注释的肿瘤组织和免疫浸润细胞的分布高度一致。（这个应该是用于第一个任务肿瘤检测吧）

作者对采用无监督学习的理由：由于我们只有幻灯片级别的标签（肿瘤或正常），监督学习并不适用于提取patch的特征。我们采用了对大规模未标注的patch进行自我监督的预训练来获得patch级的特征。在这项研究中，我们采用了对抗性对比学习AdCo[30]进行预训练。我们还测试了另一种对比性学习方法SimCLR，发现AdCo在下游任务中取得了更好的表现。

## flowchart

三个步骤。1、数据预处理（切片），2、对比学习无监督（类比simCLR，但是作者参考的是AdCo，有差异，看起来主要是利用负样本对来计算loss，等下再看看）。3、得到的representation先经过attention pooling，再用来做后续的（肿瘤检测/差异性基因表达）。我觉得我的可能更偏向于前一个任务，二分类。

![](https://halfbit.oss-cn-hangzhou.aliyuncs.com/bingli1.png)

## 数据预处理

数据集：openslide切patch，128*128，数据集：we got 14,705,914 tiles from TCGA-BRCA slides, 5,888,085 tiles from CPTAC-BRCA slides, and 12,769,300 tiles from TCGA-Lung slides.染色归一化。

## 无监督预训练

这块放到AdCo看。

## 肿瘤分类任务

采用gated-attention-pooling。通过这一步得到slide-level representation。接上全连接层、softmax。argmax获得预测概率。使用交叉熵计算slide标签和预测标签之间的loss。在下游任务期间，只有gated-attention-pooling和线性层的参数被更新，而对比学习模块的参数被冻结。

## 肿瘤分类任务结果

对于肿瘤的诊断，执行了一个分类任务来预测幻灯片级别的标签。对于乳腺癌和肺癌数据集，采用5折交叉验证来评估模型的性能。在每个折中，幻灯片被分成三个子集，80%的切片用于训练，10%用于验证，10%用于训练，10%用于验证，10%用于测试。模型的性能由5折交叉测试集的平均预测准确率来报告。在TCGA-LUNG队列中，方法达到了准确率0.963，ROC-AUC 0.976。此外，还与其他三种有竞争力的方法进行比较，包括MILRNN[33]、ABMIL[32]和DSMIL[10]。如表1所示，优于这些竞争方法至少4%的准确性。表明HistCode成功地从病理图像中提取特征用于肿瘤诊断。

![](https://halfbit.oss-cn-hangzhou.aliyuncs.com/res1.png)

此外作者比较了不同的tile级别聚合成slide级别的策略。attention pooling优于mean pool和max pool。

![](https://halfbit.oss-cn-hangzhou.aliyuncs.com/2res.png)