<!doctype html><html lang=zh-cn itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>transformer入门 - ODYSSEY</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,user-scalable=yes"><meta name=MobileOptimized content="width"><meta name=HandheldFriendly content="true"><meta name=applicable-device content="pc,mobile"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=mobile-web-app-capable content="yes"><meta name=author content="momo"><meta name=description content="nn.Transformer 最简单的实现，利用pytorch封装好的类。 参考：https://zhuanlan.zhihu.com/p/107586681 1 2 3 4 5 6"><meta name=generator content="Hugo 0.102.3"><link rel=canonical href=http://odyssey.halfbit.top/post/transformer%E5%85%A5%E9%97%A8/><link rel=icon href=/favicon.ico><link rel=stylesheet href=/sass/jane.min.fa4b2b9f31b5c6d0b683db81157a9226e17b06e61911791ab547242a4a0556f2.css integrity="sha256-+ksrnzG1xtC2g9uBFXqSJuF7BuYZEXkatUckKkoFVvI=" media=screen crossorigin=anonymous><meta property="og:title" content="transformer入门"><meta property="og:description" content="nn.Transformer 最简单的实现，利用pytorch封装好的类。 参考：https://zhuanlan.zhihu.com/p/107586681 1 2 3 4 5 6"><meta property="og:type" content="article"><meta property="og:url" content="http://odyssey.halfbit.top/post/transformer%E5%85%A5%E9%97%A8/"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-09-07T00:00:00+00:00"><meta property="article:modified_time" content="2022-09-07T00:00:00+00:00"><meta itemprop=name content="transformer入门"><meta itemprop=description content="nn.Transformer 最简单的实现，利用pytorch封装好的类。 参考：https://zhuanlan.zhihu.com/p/107586681 1 2 3 4 5 6"><meta itemprop=datePublished content="2022-09-07T00:00:00+00:00"><meta itemprop=dateModified content="2022-09-07T00:00:00+00:00"><meta itemprop=wordCount content="2306"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="transformer入门"><meta name=twitter:description content="nn.Transformer 最简单的实现，利用pytorch封装好的类。 参考：https://zhuanlan.zhihu.com/p/107586681 1 2 3 4 5 6"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>ODYSSEY</a></div><div class=mobile-navbar-icon><span></span>
<span></span>
<span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><li class=mobile-menu-item><a class=menu-item-link href=http://odyssey.halfbit.top/>Home</a></li><li class=mobile-menu-item><a class=menu-item-link href=http://odyssey.halfbit.top/post/>Archives</a></li><li class=mobile-menu-item><a class=menu-item-link href=http://odyssey.halfbit.top/categories/>Categories</a></li><li class=mobile-menu-item><a class=menu-item-link href=http://odyssey.halfbit.top/about/>About</a></li></ul></nav><link rel=stylesheet href=/lib/photoswipe/photoswipe.min.css><link rel=stylesheet href=/lib/photoswipe/default-skin/default-skin.min.css><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><header id=header class="header container"><div class=logo-wrapper><a href=/ class=logo>ODYSSEY</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=http://odyssey.halfbit.top/>Home</a></li><li class=menu-item><a class=menu-item-link href=http://odyssey.halfbit.top/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=http://odyssey.halfbit.top/categories/>Categories</a></li><li class=menu-item><a class=menu-item-link href=http://odyssey.halfbit.top/about/>About</a></li></ul></nav></header><div id=mobile-panel><main id=main class="main bg-llight"><div class=content-wrapper><div id=content class="content container"><article class="post bg-white"><header class=post-header><h1 class=post-title>transformer入门</h1><div class=post-meta><time datetime=2022-09-07 class=post-time>2022-09-07</time><div class=post-category><a href=http://odyssey.halfbit.top/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/>论文笔记</a></div></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>文章目录</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#nntransformer>nn.Transformer</a></li><li><a href=#query2label里面是如何使用transformer结构的>query2label里面是如何使用transformer结构的</a></li><li><a href=#尝试参考nntransformer源码>尝试参考nn.Transformer源码</a></li><li><a href=#encoderlayer>EncoderLayer</a></li><li><a href=#multiheadattention>MultiheadAttention</a></li><li><a href=#decoderlayer>DecoderLayer</a></li><li><a href=#总结>总结</a></li><li><a href=#最后的总结>最后的总结</a></li></ul></nav></div></div><div class=post-content><h2 id=nntransformer>nn.Transformer</h2><p>最简单的实现，利用pytorch封装好的类。</p><p>参考：https://zhuanlan.zhihu.com/p/107586681</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>transformer</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Transformer</span><span class=p>(</span><span class=n>d_model</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span> <span class=n>nhead</span><span class=o>=</span><span class=mi>8</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>src</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>rand</span><span class=p>((</span><span class=mi>49</span><span class=p>,</span> <span class=mi>12</span><span class=p>,</span> <span class=mi>512</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>tgt</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>rand</span><span class=p>((</span><span class=mi>14</span><span class=p>,</span> <span class=mi>12</span><span class=p>,</span> <span class=mi>512</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>out</span> <span class=o>=</span> <span class=n>transformer</span><span class=p>(</span><span class=n>src</span><span class=p>,</span> <span class=n>tgt</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>out</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>src是编码器的输入，tgt是解码器的输入。out的形状和tgt的形状一致。</p><h2 id=query2label里面是如何使用transformer结构的>query2label里面是如何使用transformer结构的</h2><p>1、stage1：获取backbone提取出来的特征比如，（12，2048，7，7）-> （49，12，256）</p><p>2、stage2：构建一个参数可更新的label_embedding，（14，12，256）</p><p>3、transformer（feature_maps，label_emb），形状为（14，12，256）-> （12，14，256）-> （12，14*256）</p><p>4、最后一步，经过全连接层，得到（12，num_classes）的tensor，最后是要过softmax还是sigmoid都可以，选择不同的loss。</p><p>我想要知道的，其实是对于feature_maps和label_emb这两个tensor，中间经过了怎样的黑盒子，以及论文的意思是否对得上。</p><p>所以，需要了解nn.Transformer中编码器的结构和解码器的结构。</p><p>在原文中，编码器个数为1，解码器个数为2。</p><p>能不能用到我的论文——我认为是可以的。只是我不想这样草率地去做这件事情。</p><h2 id=尝试参考nntransformer源码>尝试参考nn.Transformer源码</h2><p>了解大意即可。它是按照2017年的Attention is all you need所编写的标准的代码。</p><h2 id=encoderlayer>EncoderLayer</h2><p>由自注意力（由多头注意力机制实现）和前馈神经网络所构成。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>encoder_layer</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>TransformerEncoderLayer</span><span class=p>(</span><span class=n>d_model</span><span class=o>=</span><span class=mi>256</span><span class=p>,</span> <span class=n>nhead</span><span class=o>=</span><span class=mi>8</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>src</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>rand</span><span class=p>((</span><span class=mi>49</span><span class=p>,</span> <span class=mi>12</span><span class=p>,</span> <span class=mi>256</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>out</span> <span class=o>=</span> <span class=n>encoder_layer</span><span class=p>(</span><span class=n>src</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>out</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>输出结果为 torch.Size([49, 12, 256]) 说明输入和输出的形状是完全一致的
</span></span></span><span class=line><span class=cl><span class=s1>&#39;&#39;&#39;</span>
</span></span></code></pre></td></tr></table></div></div><p>黑盒子，里面对这个src做了哪些处理？注意，虽然这里可以穿入三个参数，src、src_mask、src_key_padding_mask但是实际上我用到的只有src，只传入了src，所以代码其实可以简化。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>src</span><span class=p>:</span> <span class=n>Tensor</span><span class=p>,</span> <span class=n>src_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span> <span class=n>src_key_padding_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=sa>r</span><span class=s2>&#34;&#34;&#34;Pass the input through the encoder layer.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Args:
</span></span></span><span class=line><span class=cl><span class=s2>        src: the sequence to the encoder layer (required).
</span></span></span><span class=line><span class=cl><span class=s2>        src_mask: the mask for the src sequence (optional).
</span></span></span><span class=line><span class=cl><span class=s2>        src_key_padding_mask: the mask for the src keys per batch (optional).
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Shape:
</span></span></span><span class=line><span class=cl><span class=s2>        see the docs in Transformer class.
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>src2</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span><span class=p>(</span><span class=n>src</span><span class=p>,</span> <span class=n>src</span><span class=p>,</span> <span class=n>src</span><span class=p>,</span> <span class=n>attn_mask</span><span class=o>=</span><span class=n>src_mask</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                          <span class=n>key_padding_mask</span><span class=o>=</span><span class=n>src_key_padding_mask</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>    src2 = self.self_attn(src,src,src) 相当于对这个自注意力层，传入了三个一摸一样的tensor
</span></span></span><span class=line><span class=cl><span class=s1>    &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>    <span class=n>src</span> <span class=o>=</span> <span class=n>src</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout1</span><span class=p>(</span><span class=n>src2</span><span class=p>)</span> <span class=c1># 这里相当于做一个残差连接吧，src+正则化以后的src2</span>
</span></span><span class=line><span class=cl>    <span class=n>src</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm1</span><span class=p>(</span><span class=n>src</span><span class=p>)</span> <span class=c1># 这里的norm都是LN</span>
</span></span><span class=line><span class=cl>    <span class=n>src2</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>linear2</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>activation</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>linear1</span><span class=p>(</span><span class=n>src</span><span class=p>))))</span> <span class=c1># 线性层、relu激活函数、dropout、线性层</span>
</span></span><span class=line><span class=cl>    <span class=n>src</span> <span class=o>=</span> <span class=n>src</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout2</span><span class=p>(</span><span class=n>src2</span><span class=p>)</span> <span class=c1># 依然是一个残差连接</span>
</span></span><span class=line><span class=cl>    <span class=n>src</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm2</span><span class=p>(</span><span class=n>src</span><span class=p>)</span> <span class=c1># LN</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>src</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=multiheadattention>MultiheadAttention</h2><p>transformer本质上最重要的应该就是这一块。多头注意力机制。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>multihead_attn</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>MultiheadAttention</span><span class=p>(</span><span class=mi>256</span><span class=p>,</span> <span class=mi>8</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>attn_output</span><span class=p>,</span> <span class=n>attn_output_weights</span> <span class=o>=</span> <span class=n>multihead_attn</span><span class=p>(</span><span class=n>src</span><span class=p>,</span><span class=n>src</span><span class=p>,</span><span class=n>src</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>src</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>attn_output</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>attn_output_weights</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>torch.Size([49, 12, 256])
</span></span></span><span class=line><span class=cl><span class=s1>torch.Size([49, 12, 256])
</span></span></span><span class=line><span class=cl><span class=s1>torch.Size([12, 49, 49])
</span></span></span><span class=line><span class=cl><span class=s1>&#39;&#39;&#39;</span>
</span></span></code></pre></td></tr></table></div></div><p>输入是上面那个src，连着3个一样的。输出，第一个是真正需要的输出，第二个应该就是权重、重要程度、关注度啥的，anyway，</p><p>输入和输出的形状是一样的。三个输入分别是q k v，里面的操作，说实话我不感兴趣了，因为我不会改里面的，所以。在编码层，就是三个一样的src传了进去，对吧？</p><h2 id=decoderlayer>DecoderLayer</h2><p>由自注意力（多头注意力机制实现）和前馈神经网络、互注意力机制（也是多头注意力机制实现）所构成。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>decoder_layer = nn.TransformerDecoderLayer(d_model=256, nhead=8)
</span></span><span class=line><span class=cl>memory = torch.rand(10, 32, 256)
</span></span><span class=line><span class=cl>tgt = torch.rand(20, 32, 256)
</span></span><span class=line><span class=cl>out = decoder_layer(tgt, memory)
</span></span><span class=line><span class=cl>print(out.shape)
</span></span></code></pre></td></tr></table></div></div><p>注意一下，这个层的输入顺序是，先tgt，再是（解码器的输出）。</p><p>输出的大小和tgt的大小是一致的。</p><p>黑盒子？做了什么？</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>tgt</span><span class=p>:</span> <span class=n>Tensor</span><span class=p>,</span> <span class=n>memory</span><span class=p>:</span> <span class=n>Tensor</span><span class=p>,</span> <span class=n>tgt_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span> <span class=n>memory_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>tgt_key_padding_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span> <span class=n>memory_key_padding_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=sa>r</span><span class=s2>&#34;&#34;&#34;Pass the inputs (and mask) through the decoder layer.
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Args:
</span></span></span><span class=line><span class=cl><span class=s2>        tgt: the sequence to the decoder layer (required).
</span></span></span><span class=line><span class=cl><span class=s2>        memory: the sequence from the last layer of the encoder (required).
</span></span></span><span class=line><span class=cl><span class=s2>        tgt_mask: the mask for the tgt sequence (optional).
</span></span></span><span class=line><span class=cl><span class=s2>        memory_mask: the mask for the memory sequence (optional).
</span></span></span><span class=line><span class=cl><span class=s2>        tgt_key_padding_mask: the mask for the tgt keys per batch (optional).
</span></span></span><span class=line><span class=cl><span class=s2>        memory_key_padding_mask: the mask for the memory keys per batch (optional).
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Shape:
</span></span></span><span class=line><span class=cl><span class=s2>        see the docs in Transformer class.
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>tgt2</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span><span class=p>(</span><span class=n>tgt</span><span class=p>,</span> <span class=n>tgt</span><span class=p>,</span> <span class=n>tgt</span><span class=p>,</span> <span class=n>attn_mask</span><span class=o>=</span><span class=n>tgt_mask</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                          <span class=n>key_padding_mask</span><span class=o>=</span><span class=n>tgt_key_padding_mask</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>tgt</span> <span class=o>=</span> <span class=n>tgt</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout1</span><span class=p>(</span><span class=n>tgt2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>tgt</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm1</span><span class=p>(</span><span class=n>tgt</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>tgt2</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>multihead_attn</span><span class=p>(</span><span class=n>tgt</span><span class=p>,</span> <span class=n>memory</span><span class=p>,</span> <span class=n>memory</span><span class=p>,</span> <span class=n>attn_mask</span><span class=o>=</span><span class=n>memory_mask</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                               <span class=n>key_padding_mask</span><span class=o>=</span><span class=n>memory_key_padding_mask</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>tgt</span> <span class=o>=</span> <span class=n>tgt</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout2</span><span class=p>(</span><span class=n>tgt2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>tgt</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm2</span><span class=p>(</span><span class=n>tgt</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>tgt2</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>linear2</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>activation</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>linear1</span><span class=p>(</span><span class=n>tgt</span><span class=p>))))</span>
</span></span><span class=line><span class=cl>    <span class=n>tgt</span> <span class=o>=</span> <span class=n>tgt</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout3</span><span class=p>(</span><span class=n>tgt2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>tgt</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm3</span><span class=p>(</span><span class=n>tgt</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>tgt</span>
</span></span></code></pre></td></tr></table></div></div><p>先是自注意力机制（扔进去tgt、tgt、tgt）实现一个tgt2，然后dropout正则化一下再残差连接；然后是LN；</p><p>然后是互注意力机制（扔进去tgt作为query，编码器的输出作为key和value）生成tgt2。</p><p>然后tgt=tgt+dropout（tgt2）</p><p>然后LN、线性层、激活、dropout、线性层，再残差连接、norm。</p><p>可以理解，其实只是说，核心区别，就是q、k、v里面的k和v换成了之前编码层的输出。</p><h2 id=总结>总结</h2><p>单独的编码器：对src*3进行自注意力操作。</p><p>单独的解码器：对tgt*3进行自注意力操作，然后对tgt、memory、memory进行互注意力操作。</p><p>编码器的堆叠：重复一样的操作。</p><p>解码器的堆叠：经过一个解码器，相当于更新了tgt；然后下一个解码器的时候，放入原来的memory和更新后的tgt。</p><p>transformer：需要定义几个编码器，几个解码器，然后对src进行编码器的操作，结束以后，这个src就作为memory，输入解码器。</p><p>每一个解码器的memory都是一样的，即key、value是不变的，变的是解码器的输入即tgt即query。它会更新。</p><h2 id=最后的总结>最后的总结</h2><p>看源码是最好的，最简洁的。</p><p>mask和pad在cv领域的transformer里面（query2label里面）是没有用到的，不用考虑。</p><p>最后再看一下query2label的插图。</p><p>可以看到query也就是tgt是会更新的。但是key&value始终是编码器（图中省略了编码器）的输出。</p><p>![截屏2022-09-07 下午12.45.41](/Users/momochan/Library/Application Support/typora-user-images/截屏2022-09-07 下午12.45.41.png)</p></div><footer class=post-footer><nav class=post-nav><a class=next href=/post/query2label/><span class="next-text nav-default">query2label</span>
<span class="prev-text nav-mobile">下一篇</span>
<i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697c-11.777231-11.500939-30.216186-10.304694-41.178865 2.712784z"/></svg></i></a></nav></footer></article></div></div></main><footer id=footer class=footer><div class=icon-links><a href=http://odyssey.halfbit.top/index.xml rel="noopener alternate" type=application/rss+xml class=iconfont title=rss target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="30" height="30"><path d="M819.157333 1024C819.157333 574.592 449.408 204.8.0 204.8V0c561.706667.0 1024 462.293333 1024 1024H819.157333zM140.416 743.04a140.8 140.8.0 01140.501333 140.586667A140.928 140.928.0 01140.074667 1024C62.72 1024 0 961.109333.0 883.626667S62.933333 743.082667 140.416 743.04zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667v-199.04c372.352.0 678.784 306.517333 678.784 678.826667z"/></svg></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme - <a class=theme-link href=https://github.com/xianmin/hugo-theme-jane>Jane</a></span>
<span class=copyright-year>&copy;
2022
<span class=heart><i class=iconfont><svg class="icon" viewBox="0 0 1025 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="14" height="14"><path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7.0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1.0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2.0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3.1-42.5-8-83.6-24-122.2z" fill="#8a8a8a"/></svg></i></span><span class=author>momoka</span></span></div></footer><div class=back-to-top id=back-to-top><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="35" height="35"><path d="M510.866688 227.694839 95.449397 629.218702h235.761562L329.15309 958.01517h362.40389L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777h894.052392v131.813095H63.840492V63.962777zm0 0"/></svg></i></div></div><script type=text/javascript src=/lib/jquery/jquery-3.2.1.min.js></script>
<script type=text/javascript src=/lib/slideout/slideout-1.0.1.min.js></script>
<script type=text/javascript src=/js/main.638251f4230630f0335d8c6748e53a96f94b72670920b60c09a56fdc8bece214.js integrity="sha256-Y4JR9CMGMPAzXYxnSOU6lvlLcmcJILYMCaVv3Ivs4hQ=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/load-photoswipe.js></script>
<script type=text/javascript src=/lib/photoswipe/photoswipe.min.js></script>
<script type=text/javascript src=/lib/photoswipe/photoswipe-ui-default.min.js></script></body></html>