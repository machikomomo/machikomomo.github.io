<!doctype html><html lang=zh-cn itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>深度学习在医学图像分析中的最新进展和临床应用 - Moka</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,user-scalable=yes"><meta name=MobileOptimized content="width"><meta name=HandheldFriendly content="true"><meta name=applicable-device content="pc,mobile"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=mobile-web-app-capable content="yes"><meta name=author content="momo"><meta name=description content="我的启发 这篇综述它比较详细地罗列了近几年半监督、无监督的分类任务上的论文。其中，基于对比学习的自监督预训练，因为作者强调了很多次，所以单独去"><meta name=generator content="Hugo 0.98.0"><link rel=canonical href=http://crepuscular.halfbit.top/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9C%A8%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F%E5%88%86%E6%9E%90%E4%B8%AD%E7%9A%84%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%95%E5%92%8C%E4%B8%B4%E5%BA%8A%E5%BA%94%E7%94%A8/><link rel=icon href=/favicon.ico><link rel=stylesheet href=/sass/jane.min.fa4b2b9f31b5c6d0b683db81157a9226e17b06e61911791ab547242a4a0556f2.css integrity="sha256-+ksrnzG1xtC2g9uBFXqSJuF7BuYZEXkatUckKkoFVvI=" media=screen crossorigin=anonymous><meta property="og:title" content="深度学习在医学图像分析中的最新进展和临床应用"><meta property="og:description" content="我的启发 这篇综述它比较详细地罗列了近几年半监督、无监督的分类任务上的论文。其中，基于对比学习的自监督预训练，因为作者强调了很多次，所以单独去"><meta property="og:type" content="article"><meta property="og:url" content="http://crepuscular.halfbit.top/post/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9C%A8%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F%E5%88%86%E6%9E%90%E4%B8%AD%E7%9A%84%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%95%E5%92%8C%E4%B8%B4%E5%BA%8A%E5%BA%94%E7%94%A8/"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-05-09T00:00:00+00:00"><meta property="article:modified_time" content="2022-05-09T00:00:00+00:00"><meta itemprop=name content="深度学习在医学图像分析中的最新进展和临床应用"><meta itemprop=description content="我的启发 这篇综述它比较详细地罗列了近几年半监督、无监督的分类任务上的论文。其中，基于对比学习的自监督预训练，因为作者强调了很多次，所以单独去"><meta itemprop=datePublished content="2022-05-09T00:00:00+00:00"><meta itemprop=dateModified content="2022-05-09T00:00:00+00:00"><meta itemprop=wordCount content="11024"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="深度学习在医学图像分析中的最新进展和临床应用"><meta name=twitter:description content="我的启发 这篇综述它比较详细地罗列了近几年半监督、无监督的分类任务上的论文。其中，基于对比学习的自监督预训练，因为作者强调了很多次，所以单独去"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>Moka</a></div><div class=mobile-navbar-icon><span></span>
<span></span>
<span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><li class=mobile-menu-item><a class=menu-item-link href=http://crepuscular.halfbit.top/>Home</a></li><li class=mobile-menu-item><a class=menu-item-link href=http://crepuscular.halfbit.top/post/>Archives</a></li><li class=mobile-menu-item><a class=menu-item-link href=http://crepuscular.halfbit.top/categories/>Categories</a></li><li class=mobile-menu-item><a class=menu-item-link href=http://crepuscular.halfbit.top/about/>About</a></li><li class=mobile-menu-item><div class=mobile-menu-parent><span class=mobile-submenu-open></span>
<a href=http://crepuscular.halfbit.top/>docs</a></div><ul class=mobile-submenu-list><li><a href=http://crepuscular.halfbit.top/post/shortcodes-preview/>Shortcodes Preview</a></li><li><a href=http://crepuscular.halfbit.top/post/image-preview/>Image Preview</a></li><li><a href=http://crepuscular.halfbit.top/post/syntax-highlighting/>Syntax Highlighting</a></li><li><a href=http://crepuscular.halfbit.top/post/math-preview/>Math Preview</a></li></ul></li></ul></nav><link rel=stylesheet href=/lib/photoswipe/photoswipe.min.css><link rel=stylesheet href=/lib/photoswipe/default-skin/default-skin.min.css><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><header id=header class="header container"><div class=logo-wrapper><a href=/ class=logo>Moka</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=http://crepuscular.halfbit.top/>Home</a></li><li class=menu-item><a class=menu-item-link href=http://crepuscular.halfbit.top/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=http://crepuscular.halfbit.top/categories/>Categories</a></li><li class=menu-item><a class=menu-item-link href=http://crepuscular.halfbit.top/about/>About</a></li><li class=menu-item><a class="menu-item-link menu-parent" href=http://crepuscular.halfbit.top/>docs</a><ul class=submenu><li><a href=http://crepuscular.halfbit.top/post/shortcodes-preview/>Shortcodes Preview</a></li><li><a href=http://crepuscular.halfbit.top/post/image-preview/>Image Preview</a></li><li><a href=http://crepuscular.halfbit.top/post/syntax-highlighting/>Syntax Highlighting</a></li><li><a href=http://crepuscular.halfbit.top/post/math-preview/>Math Preview</a></li></ul></li></ul></nav></header><div id=mobile-panel><main id=main class="main bg-llight"><div class=content-wrapper><div id=content class="content container"><article class="post bg-white"><header class=post-header><h1 class=post-title>深度学习在医学图像分析中的最新进展和临床应用</h1><div class=post-meta><time datetime=2022-05-09 class=post-time>2022-05-09</time><div class=post-category><a href=http://crepuscular.halfbit.top/categories/%E7%BB%BC%E8%BF%B0/>综述</a>
<a href=http://crepuscular.halfbit.top/categories/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F%E5%88%86%E6%9E%90/>医学图像分析</a>
<a href=http://crepuscular.halfbit.top/categories/media/>MedIA</a></div></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>文章目录</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#我的启发>我的启发</a></li><li><a href=#引用>引用</a></li><li><a href=#摘要>摘要</a></li><li><a href=#简介>简介</a></li><li><a href=#自监督学习和半监督学习>自监督学习和半监督学习</a><ul><li></li></ul></li><li><a href=#三个可以提升性能的策略>三个可以提升性能的策略</a><ul><li></li></ul></li><li><a href=#对四个任务分别具体介绍这里只写分类部分>对四个任务分别具体介绍（这里只写分类部分）</a><ul><li></li></ul></li><li><a href=#讨论>讨论</a><ul><li></li></ul></li></ul></nav></div></div><div class=post-content><h2 id=我的启发>我的启发</h2><p>这篇综述它比较详细地罗列了近几年半监督、无监督的分类任务上的论文。其中，基于对比学习的自监督预训练，因为作者强调了很多次，所以单独去找了几篇文章看了一下。既有近两年的对比自监督比较热门的文章，同时在arxiv上也看到了刚刚挂上的把对比学习自监督用于病理图像处理（包括分类和基因突变预测两个下游任务）的文章。缺点是，目前看到的，对比学习自监督预训练达到的效果，如果想要比肩甚至超过监督训练的话，需要非常大的batchsize（几千），意味着需要很强的算力。我认为这个可能是一个方向。但是如果要解决精度问题，可以再看看作者提到的解决方法。1.结合一定的标签数据，类似于半监督。2.特定的适用于医学图像的数据增强。</p><h2 id=引用>引用</h2><p>Recent advances and clinical applications of deep learning in medical image analysis,</p><p>Medical Image Analysis, 2022,</p><p><a href=https://doi.org/10.1016/j.media.2022.102444>https://doi.org/10.1016/j.media.2022.102444</a>.</p><h2 id=摘要>摘要</h2><p>这篇综述中，作者强调了最先进的无监督和半监督的深度学习在医学图像分析中的最新进展和贡献，根据不同的应用场景进行了总结，包括分类、分割、检测和图像注册。讨论了主要的技术挑战，并提出了未来研究工作中可能的解决方案。我主要关注自监督分类部分的总结。</p><h2 id=简介>简介</h2><p>与计算机视觉的常规数据集相比，医学图像数据集通常包含相对较少的图像（例如，少于10,000张），而且在许多情况下，只有一小部分图像是由专家注释的。为了克服这一限制，无监督和半监督学习方法在过去三年中受到广泛关注，这些方法能够（1）产生更多的标记图像用于模型优化，（2）从未标记的图像数据中学习有意义的隐藏模式，以及（3）为未标记的数据生成伪标签。</p><h2 id=自监督学习和半监督学习>自监督学习和半监督学习</h2><h4 id=自监督学习>自监督学习</h4><p>可以看作是深度的无监督。以无监督的方式从未标记的原始图像数据中学习丰富而有意义的特征表示，然后在各种有标记数据的下游任务中对特征表示进行微调，如分类。然而，在相当长的一段时间内，这种做法并不像NLP那样成功，相反，有监督的预训练一直是主流策略。有趣的是，作者发现这种情况在最近两年正朝着相反的方向变化，因为越来越多的研究报告显示自监督预训练的性能高于监督预训练……自监督学习有两种方式：Pretext task 和 Contrastive learning。由于基于对比学习的方法在最近几年得到了更广泛的关注，本文将强调这个方向的更多工作。其中，作者重点介绍了两个有代表性的对比学习框架，即Momentum Contrast（MoCo）（He等人，2020）和SimCLR（Chen等人，2020a）。由于自监督对比学习是非常新的技术，在本文写作时，MoCo和SimCLR等最新进展在医学图像分析领域的广泛应用还没有建立起来。尽管如此，考虑到现有文献中报道的自监督学习的可喜成果，作者预计应用这种新技术来分析医学图像的研究可能很快就会爆发。另外，自监督的预训练有很大的潜力成为监督预训练的有力替代。</p><h4 id=半监督学习>半监督学习</h4><p>半监督学习（SSL）在模型训练过程中结合了有标签和无标签的数据，可以只在无标签的数据上学习有意义的表示。特别是，半监督学习适用于有限的标记数据和大规模无标记的数据的情况下。这两类数据应该是相关的，因此，无标签数据所携带的额外信息在弥补有标签数据方面是有用的。我们将流行的SSL方法分为三组：（1）基于一致性正则化的方法；（2）基于伪标注的方法（SSL模型本身为未标记的样本生成伪标签；伪标签的样本与带有标签的样本共同用于训练SSL模型。这个过程会反复进行几次，在此期间，伪标签的质量和模型的性能会得到提高；（3）基于生成模型的方法（如GANs和VAEs。使GAN用于半监督环境的一个简单方法是通过修改判别器来执行额外的任务。例如，在图像分类的任务中，Salimans等人（2016）和Odena（2016）改变了DCGAN的判别器，强迫它作为一个分类器。对于未标记的图像，判别器的功能与vanilla GAN一样，提供输入图像是真实的概率；对于标记的图像，判别器除了产生真实性概率外，还预测其类别（这里的意思需要再修改一下）。然而，Li等人（2017）证明，两个任务的最佳性能可能无法由单一的鉴别器同时实现。因此，他们引入了一个独立于生成器和鉴别器的额外分类器。这种由三个组件组成的新架构被称为Triple-GAN。</p><p>作者罗列了无监督/半监督/自监督/全监督四个方向的医学图像分类的论文。具体看原文。</p><h2 id=三个可以提升性能的策略>三个可以提升性能的策略</h2><h4 id=注意力机制>注意力机制</h4><p>如（1）空间注意（Jaderberg等人，2015），（2）通道注意（Hu等人，2018a），（3）空间和通道注意的组合（Wang等人，2017；Woo等人，2018），以及（4）自注意（Wang等人，2018）。读者可参考Chaudhari等人（2021）的优秀评论，以了解更多关于注意力机制的细节。</p><h4 id=领域知识>领域知识</h4><p>各种领域知识，如MRI和CT图像中的解剖信息（Zhou et al., 2021, 2019a），来自体积图像的三维空间背景信息（Zhang et al., 2017; Zhuang et al, 2019；Zhu等人，2020a），同一病人的多实例数据（Azizi等人，2021），病人元数据（Vu等人，2021），放射学特征（Shorfuzzaman和Hossain，2021），图像附带的文本报告（Zhang等人，2020a）等。对如何将医学领域知识整合到网络设计中的全面回顾感兴趣的读者可以参考Xie等人（2021a）的工作。</p><h4 id=不确定性估计>不确定性估计</h4><p>当涉及到具有高安全要求的临床环境时（如癌症诊断），可靠性是关键问题。模型预测很容易受到数据噪声和推理误差等因素的影响，所以最好能量化不确定性，使结果值得信赖（Abdar等人，2021）。常用的不确定性估计技术包括贝叶斯近似法（Gal和Ghahramani，2016）和模型集合法（Lakshminarayanan等人，2017）。贝叶斯方法，如蒙特卡洛drop-out（MC-drop-out）（Gal和Ghahramani，2016），围绕着对神经网络参数的后验分布进行近似。集合技术结合多个模型来测量不确定性。对不确定性估计感兴趣的读者请参考Abdar等人（2021）的综合评论。</p><h2 id=对四个任务分别具体介绍这里只写分类部分>对四个任务分别具体介绍（这里只写分类部分）</h2><h4 id=自监督分类>自监督分类</h4><p>基于自监督学习的分类。最近的自我监督学习方法在提高缺乏足够注释的医疗任务的性能方面显示出巨大的潜力（Bai等人，2019；Tao等人，2020；Li等人，2020a；Shorfuzzaman和Hossain，2021；Zhang等人，2020a）。这种方法适用于有大量医学图像，但只有一小部分有标签的情况。因此，模型优化分为两个步骤，即自我监督的预训练和监督的微调。模型最初使用未标记的图像进行优化，以有效学习代表图像语义的良好特征（Azizi等人，2021）。来自自我监督的预训练模型在监督下进行微调，以便在随后的分类任务中获得更快、更好的性能（Chen等人，2020c）。在实践中，自我监督可以通过Pretext task（Misra和Maaten，2020）或对比学习（Jing和Tian，2020）创建，具体如下。</p><p>Pretext task：如旋转预测（Tajbakhsh等人，2019）和魔方恢复（Zhuang等人，2019；Zhu等人，2020a）。Chen等人（2019b）认为，现有的预文任务，如相对位置预测（Doersch等人，2015）和局部语境预测（Pathak等人，2016）在医学图像数据集上只带来了边际改善；作者设计了一个基于语境恢复的新Pretext task。这个新的预语任务有两个步骤：在损坏的图像中打乱斑块，并恢复原始图像。上下文修复的预训练策略提高了医学图像分类的性能。Tajbakhsh等人（2019年）利用三个预案任务，即旋转（Gidaris等人，2018年）、着色（Larsson等人，2017年）和基于WGAN的补丁重建，为分类任务预训练模型。在预训练后，模型使用标记的例子进行训练。研究表明，在医学领域，基于Pretext task的预训练比随机初始化和转移学习（ImageNet预训练）对糖尿病视网膜病变分类更有效。</p><p>contrastive learning：Azizi等人（2021）采用自监督学习框架SimCLR（Chen等人，2020a）来训练模型（ResNet-50和ResNet-152的宽泛版本），用于皮肤病病情分类和胸部X光分类。他们首先使用未标记的自然图像，然后使用未标记的皮肤病图像和胸部X光片对模型进行预训练。通过最大限度地提高正面图像对之间的一致性来学习特征表示，这些图像对要么是同一图像的两个增强的例子，要么是来自同一病人的多个图像。使用更少的已标记的皮肤学图像和胸部X光片对预训练的模型进行了微调。这些模型在胸部X射线分类中的平均AUC比使用ImageNet预训练的模型高出1.1%，在皮肤病条件分类中的最高准确率高出6.7%。MoCo（He等人，2020；Chen等人，2020b）是另一个流行的自我监督学习框架，用于预训练医学分类任务的模型，如CT图像的COVID-19诊断（Chen等人，2021a）和胸部X射线的胸腔积液识别（Sowrirajan等人，2021）。此外，事实证明，自我监督的对比性预训练可以大大受益于领域知识的纳入。例如，Vu等人（2021年）利用病人元数据（病人编号、图像横向和研究编号），从多个胸部X光图像中构建和选择阳性对，用于MoCo预训练。在只有1%的胸腔积液分类标签数据的情况下，与之前的对比学习方法（Sowrirajan等人，2021）和ImageNet预训练相比，所提出的方法分别提高了平均AUC 3.4%和14.4%。</p><h4 id=半监督分类>半监督分类</h4><p>与自监督方法不同的是，自监督方法可以仅仅从未标记的数据中学习有用的特征表征，而半监督学习需要通过不同的方式将未标记的数据与标记的数据整合在一起，以训练模型获得更好的性能。Madani等人（2018a）采用了以半监督方式训练的GAN（Kingma等人，2014），用于胸部X光片中的心脏病分类，其中标注的数据是有限的。与vanilla GAN（Goodfellow等人，2014年）不同，这个半监督的GAN是用未标记和标记的数据来训练的。它的判别器经过修改，不仅可以预测输入图像的真实性，还可以预测真实数据的图像类别（正常/非正常）。当增加标记实例的数量时，基于半监督的GAN的分类器始终比监督的CNN表现更好。半监督的GAN也被证明在其他数据有限的分类任务中很有用，如CT肺结节分类（Xie等人，2019a），以及超声心动图的左心室肥大分类（Madani等人，2018b）。除了半监督对抗方法，基于一致性的半监督方法，如Π -Model（Laine和Aila，2017）和Mean Teacher（Tarvainen和Valpola，2017）也被用于利用未标记的医学图像数据进行更好的分类（Shang等，2019; Liu等，2020a）。</p><h2 id=讨论>讨论</h2><h4 id=作者关于特定任务的观点>作者关于特定任务的观点</h4><p>利用深度学习进行医学图像分析的进展遵循滞后但与计算机视觉相似的时间表。然而，由于医学图像和自然图像之间的差异，直接使用计算机视觉的方法可能不会产生令人满意的结果。为了达到良好的性能，需要解决医学成像任务所特有的挑战。对于分类任务来说，成功的关键在于提取与某些类别有关的高度鉴别性特征。这对于类间差异大的领域来说相对容易（例如，许多公共胸部X光数据集的准确率通常超过90%），但对于类间相似度高的领域来说，这可能是困难的。例如，乳房X光片分类的性能总体上不是很好（例如，在私人数据集上常见70∼80%的准确率），因为在存在重叠、异质的纤维腺组织的情况下，很难捕捉到乳腺肿瘤的鉴别特征（Geras等人，2019）。<strong>细粒度视觉分类（FGVC）的概念（Yang等人，2018），旨在识别视觉上相似物体之间的细微差别，可能适合学习给定的高类间相似度的独特特征。但请注意，基准FVGC数据集是特意收集的，以使所有图像样本一致表现出高类间相似性。因此，在这些数据集上开发和评估的方法可能不容易适用于医疗数据集，因为在这些数据集上，只有一部分而不是所有的图像表现出高类间相似性。尽管如此，我们相信FVGC方法如果经过适当的修改，对于学习具有高判别力的医学图像分类的特征表示是有价值的。其他可能提高特征识别能力的方法包括使用注意力模块、局部和全局特征、领域知识等。</strong></p><p>医学对象检测比分类更复杂，这一点可以从边界盒预测的过程中看出来。自然地，检测面临着分类所固有的挑战。同时，还存在额外的挑战，特别是小规模物体的检测（如肺部小结节）和类的不平衡。一阶段检测器在检测大型物体时通常表现得与两阶段检测器相当，但在检测小型物体时却更加挣扎。现有的研究表明，使用多尺度特征可以大大缓解单阶段和两阶段检测器的这个问题。一个简单而有效的方法是特征化图像金字塔（Liu等人，2020b），即从同一图像的多个尺度上独立提取特征。这种方法可以帮助放大小物体以达到更好的性能，但计算成本高且速度慢。尽管如此，它还是适用于对速度没有要求的医学检测任务。另一种有用但更快的方法是特征金字塔，它利用了不同卷积层的多尺度特征图。尽管存在各种构建特征金字塔的方法，但经验法则是有必要将强大的高级语义与高分辨率的特征图相融合。正如FPN（Lin等人，2017a）所示，这在检测小物体方面发挥了重要作用。</p><p>类不平衡产生于这样一个事实：检测器需要评估大量的候选区域，但只有少数区域包含感兴趣的物体。换句话说，类别平衡严重偏向于负面的例子（如背景区域），其中大部分是容易否定的例子。大量容易否定的例子的存在会使训练过程不堪重负，导致检测结果不佳。两阶段检测器可以比单阶段检测器更好地处理这种类别不平衡问题，因为大多数负面建议在区域建议阶段就被过滤掉了。就单阶段检测器而言，最近的研究表明，放弃对锚箱的主导使用可以在很大程度上缓解类不平衡（Duan等人，2019）。然而，医学对象检测中采用的大多数方法仍然是基于锚点的。在不久的将来，我们期望在医学对象检测中看到更多对无锚、单阶段检测器的探索。</p><p>医学图像分割结合了分类和检测的挑战。就像检测一样，类别不平衡是二维和三维医学分割任务中的一个共同问题。另一个类似的挑战是小尺寸病变（如MRI多发性硬化症）和器官（如腹部CT扫描的胰腺）的分割。而且，这两个挑战经常出现交织在一起。这些问题在很大程度上通过调整指标/损失来评估分割性能得到了缓解，如Dice系数（Milletari等人，2016）、广义Dice（Sudre等人，2017）、病灶损失的整合（Abraham和Khan，2019）等。然而，这些指标是基于区域的（即分割误差是以像素方式计算的）。这可能导致有关结构、形状和轮廓的宝贵信息的损失，而这些信息对后期的诊断/预后很重要。因此，我们认为有必要开发非基于区域的指标，为基于区域的指标提供补充信息以提高分割性能。目前在这个方向上只有少数研究（Kervadec等人，2019）。我们期望在未来能看到更多。</p><p>此外，诸如纳入局部和全局背景、注意机制、多尺度特征和解剖学线索等策略一般都有利于提高大物体和小物体的分割精度。在这里，我们想强调transformer的巨大潜力，因为它具有强大的长距离依赖性建模能力。虽然长距离的依赖关系有助于实现精确的分割，但大多数基于CNN的方法并没有明确关注这一方面。依赖关系大致有两种类型，即片内依赖关系（CT或MRI片内的像素关系）和片间依赖关系（CT或MRI片之间的像素关系）（Li等人，2020e）。最近的研究表明，基于变换器的方法在这两种情况下都很强大（Chen等人，2021b；Valanarasu等人，2021）。<strong>transformer在医学图像分割尤其是三维图像分割方面的应用仍处于初始阶段，这一试验中的更多作品可能很快就会涌现。</strong></p><h4 id=从不同学习范式的角度来看>从不同学习范式的角度来看</h4><p>尽管深度学习在放射学图像分析的不同任务中带来了巨大的成功，但进一步提高性能的主要障碍是需要大量的注释数据集。监督转移学习可以大大缓解这个问题，通过用在相关/不相关数据集（如ImageNet）上预训练的模型的权重来初始化模型的权重（针对目标任务）。除了广泛使用的转移学习，还有两个可能的方向：（1）利用GAN模型来扩大标记的数据集；（2）利用自我监督和半监督的学习模型来利用大量未标记的医学图像的潜在信息。</p><p>GAN在医学图像合成和半监督学习中显示出巨大的前景；但一个挑战是如何在GAN的生成器和目标任务（如分类器、检测器、分割器）之间建立一个强大的连接。与传统的数据增强（如旋转、重新缩放和翻转）相比，缺乏这种联系可能会导致微妙的性能提升（Wu et al., 2018a）。生成器和分类器之间的联系可以通过利用半监督的GAN来加强，其中判别器被修改为分类器（Salimans等人，2016）。还可以采用几种训练策略：识别一个 &ldquo;坏 &ldquo;的生成器，它可以极大地促进良好的半监督分类（Dai等人，2017）；联合优化生成器、判别器和分类器的三重成分（Li等人，2017）。探索新的方法是有意义的，可以有效地在发生器和特定的医学图像任务之间建立联系，以获得更好的性能。此外，GAN通常需要至少数千个训练实例才能收敛，这限制了它在小型医疗数据集上的适用性。这一挑战可以通过使用经典的数据增量进行对抗性学习来部分解决（Frid-Adar等人，2018a，2018b）。此外，如果存在相对大量的与目标数据集具有结构、纹理和语义相似性的医学图像，预训练生成器和/或判别器可能会促进更快的收敛和更好的性能（Rubin等人，2019）。<strong>同时，最近一些新的增强机制，如可分化增强（Zhao等人，2020）和自适应判别器增强（Karras等人，2020）使GAN在数据有限的条件下有效地生成高保真图像，但它们还没有被应用于任何医学图像分析任务。我们预计，这些新方法在未来的医学图像分析领域的研究中也能表现出很好的性能。</strong></p><p>自我监督可以通过借口任务或对比学习来构建，但后者似乎是一个更有前途的研究方向。这是因为，一方面，直接使用计算机视觉中的借口任务（如七巧板）通常不足以确保学习放射学图像的强大特征表示。另一方面，设计新的借口任务可能是困难的，这需要精细的操作。与其设计各种借口任务，自我监督的对比学习通过迫使网络对不同的增强视图保持不变，来训练网络捕捉有意义的特征表征，这有可能在不同的下游任务上胜过监督的转移学习，如医学图像分类和分割。尽管自监督对比学习的表现令人鼓舞，但其在放射学图像分析中的应用仍处于探索阶段，如何适当利用这种新的学习范式是一个难题。为了释放其潜力，我们在此从以下三个方面提出建议。(1) 利用对比学习和监督学习的优势。从现有的研究中，我们发现大多数的医学图像分析都采用了两个独立的步骤：在未标记的数据上进行对比性的预训练和在标记的数据上进行监督性的微调。在预训练阶段，大多数研究都依赖于相对较大的无标签数据集，以确保学习高质量、可转移的特征，这些特征在使用有限的标签数据进行微调后可以产生卓越的性能。然而，在缺乏大量无标签数据的任务中，对大型无标签数据的依赖可能会产生问题。为了扩大应用范围，用较少的无标签数据学习高质量的特征展示将是可取的。一种可能的方法是将前面提到的两个独立的步骤统一为一个步骤，这样标签信息就可以在对比学习中得到利用。这有点让人联想到半监督式学习，即同时利用未标注和标注的数据来达到更好的效果。<strong>更具体地说，类别标签可以用来指导以更紧凑的方式构建正反两方面的配对，推动同一类别的图像在低维表示空间中更紧密地排列（Khosla等人，2020）。（这里没看懂作者的意思，感觉像是半监督学习。马这篇文章。）<strong>与仅通过自我监督学习（即没有任何类别标签）学习的特征相比，以这种方式学习的特征应该需要更少的无标签数据，而且冗余度更低。(2) 考虑到对比性学习的某些特性以获得更好的性能。例如，一项研究证明，对比性学习从大块的相似点中获益更多，而不是成对的（Saunshi等人，2019）。这种启发式方法可能很适合从表现出连续解剖学相似性的三维CT和MRI卷中学习可转移的特征。</strong>（应该是指序列信息？）</strong>(3) 为对增强敏感的下游任务定制数据增强策略。事实证明，在大多数现有的对比学习框架中，不同的数据增强策略的组成对学习代表性特征至关重要。例如，SimCLR对无标签的图像进行了三种类型的转换，即随机裁剪、颜色扭曲和高斯模糊（Chen等人，2020a）。然而，一些常用的增强技术可能并不适用于医学图像。在放射学中，大多数图像是以灰度呈现的，颜色失真策略可能不适合。另外，在未标记的医学图像的细部细节带有重要信息的情况下，应用高斯模糊可能会破坏细节信息，并在预训练阶段降低特征表示的质量。<strong>因此，选择适当的数据增强策略以确保满意的下游性能是很重要的。此外，自监督对比性预训练目前受到大型模型（如ResNet-50（4×），ResNet-152（2×））的高计算复杂性的阻碍，这需要一大批多核TPU（Chen等人，2020a）。因此，开发新的模型或训练策略以提高计算效率应该是一个重要方向。例如，Reed等人（2022）提出了一种分层预训练策略，使自监督预训练过程的收敛速度提高到80×，并在不同的任务中提高了准确性。</strong></p><p>与自监督对比学习一样，最近的半监督方法，如FixMatch（Sohn等人，2020）严重依赖先进的数据增强策略来实现良好的性能。为了促进半监督学习在医学图像分析中的应用，有必要以数据集驱动和/或任务驱动的方式开发适当的增强策略。数据集驱动 &ldquo;意味着为感兴趣的特定数据集找到最佳的增强策略。在过去，由于参数搜索空间非常大（如Cubuk等人（2020）所显示的1034种可能的增强策略），这并不容易实现。最近，像RandAugment（Cubuk等人，2020）这样的自动数据增强策略已经被提出来，以大大减少搜索空间。**然而，在医学图像分析中，自动增强的概念在很大程度上仍未被发掘。作为 &ldquo;任务驱动&rdquo;，意味着为一个特定的任务（如MRI前列腺分割）找到合适的增强策略，这些任务有几个数据集。**这可以被看作是数据集驱动的增强的延伸，因此更具挑战性，但它可以帮助在一个数据集上开发的算法更好地推广到同一任务的其他数据集。</p><p>另一个问题是，由于违反了半监督学习的基本假设&ndash;标记的和未标记的数据来自同一分布，可能会造成性能下降**（领域适应）**。事实上，当半监督方法被应用于医学图像分析时，分布不匹配是一个常见的问题。考虑以下例子：在从CT切片中分割COVID-19肺部感染的任务中，假设你有一组包含相对平衡的感染和非感染切片的有标签的CT卷，而可用的无标签的CT卷可能不包含或只包含几个感染的切片。或者未标记的CT图像不仅包含COVID-19感染，还包含其他一些疾病类别（如结核病），而这些疾病在标记的图像中是没有的。如果未标记数据的分布与标记数据的分布不匹配，会发生什么？现有的研究表明，这将导致半监督方法的性能急剧下降，有时甚至比简单的监督基线更差（Oliver等人，2018；Guo等人，2020）。因此，有必要对半监督算法进行调整，使其能够容忍标记的和未标记的医疗数据之间的分布不匹配。作为一个相关领域，&ldquo;领域适应 &ldquo;可能为实现这一目标提供见解。</p><h4 id=寻找更好的架构和pipeline>寻找更好的架构和pipeline</h4><p>深度学习在医学图像分析中的持续成功不仅源于不同的学习范式（无监督、半监督），而且，也许在更大程度上，还源于随着时间推移提出的架构/模型。回顾过去，我们发现非微不足道的改进与 &ldquo;架构 &ldquo;的进步密切相关，例子包括AlexNet（Krizhevsky等人，2012）、残余连接（He等人，2016）、跳过连接（Ronneberger等人，2015）、自我关注（Dosovitskiy等人，2020）等等。&ldquo;鉴于这种进展历史，当然有可能更好的神经结构本身就能克服目前的许多限制&rdquo;，正如Yuille和Liu（2021）所指出的。我们剖析了可能有助于寻找更好的架构的两个方面。首先，生物和认知上的启发机制将继续在架构设计中发挥重要作用。深度学习神经网络最初是受大脑背景的架构启发。近年来，受灵长类动物视觉注意力机制启发的注意力概念已被成功用于NLP和计算机视觉，使模型专注于输入数据的重要部分，从而获得卓越的性能。一个突出的例子是基于自我注意的transformer系列（Dosovitskiy等人，2020）。与基于CNN的主流模型相比，基于Transformer的架构更善于捕捉输入和输出序列之间的全局/长程依赖关系。另外，CNN固有的归纳偏见（如翻译等价性和定位性）在Transformer中要少得多（Dosovitskiy等人，2020）。除了注意力机制外，许多其他生物或认知机制，如人类语言中的动态层次，对新对象和概念的一次性学习而不需要梯度下降等（Marblestone等人，2016），可能为设计更强大的自动架构提供灵感。第二，自动架构工程可以为开发更好的架构提供启示。目前采用的架构师大多来自人类专家，而设计过程是反复的，容易出错。部分原因是，用于医学图像分析的模型主要是由计算机视觉中开发的模型改编的。为了避免人工设计的需要，研究人员提出了架构工程的自动化，一个相关的领域是神经架构搜索（NAS）（Zoph and Le, 2017）。然而，大多数现有的NAS研究都局限于图像分类（Elsken等人，2019），真正能够带来根本性变化的革命性模型还没有从这个过程中产生（Yuille和Liu，2021）。尽管如此，NAS仍然是一个值得探索的方向。</p><p>目前，基于深度学习的管道通常涉及几个相互依赖的子组件，如图像预处理和后处理，适应和训练网络架构，选择适当的损失，数据增强方法等。但设计选择往往太多，实验者无法手动找出一个最佳管道。此外，为一个特定任务的数据集（如来自一家医院的CT图像）配置的高性能管道可能在同一任务的另一个数据集（如来自不同医院的CT图像）上表现不佳。因此，需要能够自动配置其子组件的管道来加速经验设计。属于这个范围的例子包括NiftyNet（Gibson等人，2018b），一个用于不同医学应用的模块化管道，以及专门用于医学图像分割的nnU-Net（Isensee等人，2021）。我们预计这个方向会有更多的研究出来。</p><h4 id=融入领域知识>融入领域知识</h4><p>例如MRI和CT图像中的解剖信息（Zhou等人，2021，2019a），来自同一病人的多实例数据（Azizi等人，2021），病人元数据（Vu等人，2021），放射学特征，以及图像附带的文本报告（Zhang等人，2020a）。另一方面，我们观察到，有效地纳入放射科医生熟悉的强大领域知识可能更难。一个例子是从乳房X光照片中识别乳腺癌。对于每个病人，有四张乳房X光照片，包括两张颅底（CC）和两张左（L）和右（R）乳房的中侧斜面（MLO）视图图像。在临床实践中，双侧差异（如LCC与RCC）和单侧对应（如LCC和LMLO）是放射科医生检测可疑区域和确定恶性的重要线索。目前，很少有方法能可靠和准确地利用这种专家知识。因此，需要更多的研究工作来最大限度地利用强大的领域知识。</p></div><footer class=post-footer><nav class=post-nav><a class=prev href=/post/simclr%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417l277.93508-310.326815c11.338233-12.190647 11.035334-32.285311-.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"/></svg></i><span class="prev-text nav-default">SimCLR论文笔记</span>
<span class="prev-text nav-mobile">上一篇</span></a>
<a class=next href=/post/math-preview/><span class="next-text nav-default">Math Preview</span>
<span class="prev-text nav-mobile">下一篇</span>
<i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697c-11.777231-11.500939-30.216186-10.304694-41.178865 2.712784z"/></svg></i></a></nav></footer></article></div></div></main><footer id=footer class=footer><div class=icon-links><a href=http://crepuscular.halfbit.top/index.xml rel="noopener alternate" type=application/rss+xml class=iconfont title=rss target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="30" height="30"><path d="M819.157333 1024C819.157333 574.592 449.408 204.8.0 204.8V0c561.706667.0 1024 462.293333 1024 1024H819.157333zM140.416 743.04a140.8 140.8.0 01140.501333 140.586667A140.928 140.928.0 01140.074667 1024C62.72 1024 0 961.109333.0 883.626667S62.933333 743.082667 140.416 743.04zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667v-199.04c372.352.0 678.784 306.517333 678.784 678.826667z"/></svg></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme - <a class=theme-link href=https://github.com/xianmin/hugo-theme-jane>Jane</a></span>
<span class=copyright-year>&copy;
2022
<span class=heart><i class=iconfont><svg class="icon" viewBox="0 0 1025 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="14" height="14"><path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7.0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1.0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2.0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3.1-42.5-8-83.6-24-122.2z" fill="#8a8a8a"/></svg></i></span><span class=author>momoka</span></span></div></footer><div class=back-to-top id=back-to-top><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="35" height="35"><path d="M510.866688 227.694839 95.449397 629.218702h235.761562L329.15309 958.01517h362.40389L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777h894.052392v131.813095H63.840492V63.962777zm0 0"/></svg></i></div></div><script type=text/javascript src=/lib/jquery/jquery-3.2.1.min.js></script>
<script type=text/javascript src=/lib/slideout/slideout-1.0.1.min.js></script>
<script type=text/javascript src=/js/main.638251f4230630f0335d8c6748e53a96f94b72670920b60c09a56fdc8bece214.js integrity="sha256-Y4JR9CMGMPAzXYxnSOU6lvlLcmcJILYMCaVv3Ivs4hQ=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/load-photoswipe.js></script>
<script type=text/javascript src=/lib/photoswipe/photoswipe.min.js></script>
<script type=text/javascript src=/lib/photoswipe/photoswipe-ui-default.min.js></script></body></html>