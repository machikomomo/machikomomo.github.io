<!doctype html><html lang=zh-cn itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>MAG-SD论文笔记 - ODYSSEY</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,user-scalable=yes"><meta name=MobileOptimized content="width"><meta name=HandheldFriendly content="true"><meta name=applicable-device content="pc,mobile"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=mobile-web-app-capable content="yes"><meta name=author content="momo"><meta name=description content="MAG-SD https://github.com/lijx1996/MAG-SD https://ieeexplore.ieee.org/abstract/document/9351607 2021 JBHI 我的启发 之前对WS-DAN和MAG-SD做过一个比较。这次先回顾一下之前记录的比较，然后再重新读一下这篇文章并写笔记（主要是看一下"><meta name=generator content="Hugo 0.102.3"><link rel=canonical href=http://odyssey.halfbit.top/post/mag-sd%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/><link rel=icon href=/favicon.ico><link rel=stylesheet href=/sass/jane.min.fa4b2b9f31b5c6d0b683db81157a9226e17b06e61911791ab547242a4a0556f2.css integrity="sha256-+ksrnzG1xtC2g9uBFXqSJuF7BuYZEXkatUckKkoFVvI=" media=screen crossorigin=anonymous><meta property="og:title" content="MAG-SD论文笔记"><meta property="og:description" content="MAG-SD https://github.com/lijx1996/MAG-SD https://ieeexplore.ieee.org/abstract/document/9351607 2021 JBHI 我的启发 之前对WS-DAN和MAG-SD做过一个比较。这次先回顾一下之前记录的比较，然后再重新读一下这篇文章并写笔记（主要是看一下"><meta property="og:type" content="article"><meta property="og:url" content="http://odyssey.halfbit.top/post/mag-sd%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-05-24T00:00:00+00:00"><meta property="article:modified_time" content="2022-05-24T00:00:00+00:00"><meta itemprop=name content="MAG-SD论文笔记"><meta itemprop=description content="MAG-SD https://github.com/lijx1996/MAG-SD https://ieeexplore.ieee.org/abstract/document/9351607 2021 JBHI 我的启发 之前对WS-DAN和MAG-SD做过一个比较。这次先回顾一下之前记录的比较，然后再重新读一下这篇文章并写笔记（主要是看一下"><meta itemprop=datePublished content="2022-05-24T00:00:00+00:00"><meta itemprop=dateModified content="2022-05-24T00:00:00+00:00"><meta itemprop=wordCount content="2597"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="MAG-SD论文笔记"><meta name=twitter:description content="MAG-SD https://github.com/lijx1996/MAG-SD https://ieeexplore.ieee.org/abstract/document/9351607 2021 JBHI 我的启发 之前对WS-DAN和MAG-SD做过一个比较。这次先回顾一下之前记录的比较，然后再重新读一下这篇文章并写笔记（主要是看一下"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>ODYSSEY</a></div><div class=mobile-navbar-icon><span></span>
<span></span>
<span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><li class=mobile-menu-item><a class=menu-item-link href=http://odyssey.halfbit.top/>Home</a></li><li class=mobile-menu-item><a class=menu-item-link href=http://odyssey.halfbit.top/post/>Archives</a></li><li class=mobile-menu-item><a class=menu-item-link href=http://odyssey.halfbit.top/categories/>Categories</a></li><li class=mobile-menu-item><a class=menu-item-link href=http://odyssey.halfbit.top/about/>About</a></li></ul></nav><link rel=stylesheet href=/lib/photoswipe/photoswipe.min.css><link rel=stylesheet href=/lib/photoswipe/default-skin/default-skin.min.css><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><header id=header class="header container"><div class=logo-wrapper><a href=/ class=logo>ODYSSEY</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=http://odyssey.halfbit.top/>Home</a></li><li class=menu-item><a class=menu-item-link href=http://odyssey.halfbit.top/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=http://odyssey.halfbit.top/categories/>Categories</a></li><li class=menu-item><a class=menu-item-link href=http://odyssey.halfbit.top/about/>About</a></li></ul></nav></header><div id=mobile-panel><main id=main class="main bg-llight"><div class=content-wrapper><div id=content class="content container"><article class="post bg-white"><header class=post-header><h1 class=post-title>MAG-SD论文笔记</h1><div class=post-meta><time datetime=2022-05-24 class=post-time>2022-05-24</time><div class=post-category><a href=http://odyssey.halfbit.top/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/>论文笔记</a>
<a href=http://odyssey.halfbit.top/categories/jbhi/>JBHI</a></div></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>文章目录</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#我的启发>我的启发</a></li><li><a href=#比较>比较</a></li><li><a href=#摘要>摘要</a></li><li><a href=#介绍>介绍</a></li><li><a href=#三个贡献>三个贡献</a></li><li><a href=#related-work>Related Work</a></li><li><a href=#method>Method</a></li><li><a href=#architecture>Architecture</a></li><li><a href=#attention-pooling>Attention pooling</a></li><li><a href=#features-attention>Features->Attention</a></li><li><a href=#代码>代码</a></li><li><a href=#过拟合>过拟合</a></li><li><a href=#test>Test</a></li></ul></nav></div></div><div class=post-content><h1 id=mag-sd>MAG-SD</h1><p><a href=https://github.com/lijx1996/MAG-SD>https://github.com/lijx1996/MAG-SD</a></p><p><a href=https://ieeexplore.ieee.org/abstract/document/9351607>https://ieeexplore.ieee.org/abstract/document/9351607</a></p><p>2021 JBHI</p><h2 id=我的启发>我的启发</h2><p>之前对WS-DAN和MAG-SD做过一个比较。这次先回顾一下之前记录的比较，然后再重新读一下这篇文章并写笔记（主要是看一下它的框架是怎么修改的）。最后看代码。</p><h2 id=比较>比较</h2><p>1、特征图的多尺度。使用ResNet50作为backbone提取出不同尺寸的feature map——f1，f2，f3。生成对应的a1、a2、a3。（For ResNet50 we used, feature maps with 512 ∗ 28 ∗ 28, 1024 ∗ 14 ∗ 14, 2048 ∗ 7 ∗ 7 sizes are chosen. The number of attention map is 32.）</p><p>2、WSDAN的attention map引导的数据增强有crop和drop。MA提出mix-up（大于阈值的区域记为1，画bounding box，resize成原图大小，两者按照一定比例相加合并、patching（复制）、dimming（类似drop）</p><p>3、loss。WSDAN的attention regularization loss修改为Soft Distance Regularization using P, p 1, p 2, p 3to calculate overall loss。</p><h2 id=摘要>摘要</h2><p>任务：利用CXR图像将COVID-19从肺炎病例中分类。</p><p>挑战：shared spatial characteristics, high feature variation and contrast diversity between cases. 病例之间具有共同的空间特征、高特征变化和对比度多样性。以及数据少。Moreover, massive data collection is impractical for a newly emerged disease, which limited the performance of data thirsty deep learning models.</p><p>关于名字：Multiscale Attention Guided deep network with Soft Distance regularization (MAG-SD) is proposed to automatically classify COVID-19 from pneumonia CXR images.</p><p>在MAG-SD中，MA-Net被用来从多尺度特征图中产生预测向量和注意力。为了提高训练模型的鲁棒性和缓解训练数据的不足，提出了注意力引导的增强和软距离正则化，其目的是产生有意义的增强并减少噪音。</p><h2 id=介绍>介绍</h2><p>关于任务的医学背景，这个任务和人工智能辅助诊疗的可能性，以及现有的工作，数据集的来源。</p><p><strong>存在的问题：<strong>一般来说，目前在CXR图像上操作的研究大多依赖于在线数据集和有限的COVID-19病例。不足的数据难以评估模型的稳健性，并限制了其普遍性。在极其不平衡的数据集上训练的模型也导致了长尾分布问题。尽管有很多作品讨论了通过人工智能诊断COVID-19的问题，但由于几个问题，很少有作品解决</strong>不平衡数据和数据集的有限规模问题</strong>。1）由不平衡数据训练的模型倾向于将所有目标分类到主导类，而主导类的标签绝大多数都比其他类多。2）X射线图像上的独特标签，如L/R位置标签，很容易引起模型的注意，然后误导预测。3）COVID-19病例与非COVID病例有共同的特征，这就要求有一个敏感和强大的模型来进行分类。</p><h2 id=三个贡献>三个贡献</h2><p>1）设计了一个新的深度网络，MA-Net，将COVID19的诊断作为一个FGVC问题。引入了多尺度注意来评估多层次特征的注意图。组成的注意图被用作训练步骤的指导。提出了注意力集合，以利用注意力图进行分类。</p><p>2）通过提出注意力引导的数据增强和多镜头训练阶段来解决数据短缺问题。它包括注意混合、注意修补和注意调光，可以增强和搜索局部特征，然后生成数据。模型在不平衡的COVID-19数据集上进行了训练，并达到了最先进的水平。</p><p>3）在不引入其他模块或参数的情况下，制定了一个新的正则化术语，利用预测之间的软距离，作为一个约束条件，限制分类器对一个目标产生矛盾的输出。</p><h2 id=related-work>Related Work</h2><p>肺炎X射线图像、细粒度的视觉分类、CNN的注意机制和计算机视觉中利用的多尺度特征融合。</p><p>多尺度特征融合：从多分辨率的输入图像中提取混合特征图是自手工设计特征的时代以来计算机视觉中的一个常见策略。CNN具有固有的金字塔形状的多尺度特征，如果进行有效的特征融合，在产生强大的语义表征方面是很有利的。U-Net[37]和V-Net[38]等模型利用了跳过连接来关联不同分辨率的特征图。FPN[39]利用多尺度层次的预测，产生了多个预测。对于CXR图像，Huang等人[40]提出了权重串联的方法来合作全局和局部特征。空间注意力的蓬勃发展给了人们从多分辨率特征图中提取注意力的灵感。Sedai等人[41]提出了用于胸部病变定位的A-CNN，该方法通过计算特征图的加权平均数进行多尺度的注意。</p><h2 id=method>Method</h2><p>The proposed attention generating model is operated on multiscale feature maps, aiming at extracting attention from different scale. Layers before downsampling are selected as feature map in order to squeeze information out of single resolution feature. For ResNet50 we used, feature maps with 512 ∗ 28 ∗ 28, 1024 ∗ 14 ∗ 14, 2048 ∗ 7 ∗ 7 sizes are chosen. The number of attention map is 32.</p><p>多尺度注意力生成器的结构如图3所示。f 1、f 2和f 3是由特征提取器选择的特征图。它们中的每一个都被1∗1卷积处理以产生相应的注意力。所有的注意力图都被下采样为7∗7，并进行剩余连接。实验中讨论了使用不同数量的特征图的效果。</p><h2 id=architecture>Architecture</h2><p><img src=https://halfbit.oss-cn-hangzhou.aliyuncs.com/2022-05-243.48.40.png alt></p><p>对照上图，代码里要看的地方：</p><p>1.分别从哪里提取features，features转为attention的方式。</p><p>2.soft的实现</p><p>3.features和attention融合的方式（attention pooling）</p><h2 id=attention-pooling>Attention pooling</h2><p><img src=https://halfbit.oss-cn-hangzhou.aliyuncs.com/2022-05-243.52.34.png alt></p><p><img src=https://halfbit.oss-cn-hangzhou.aliyuncs.com/2022-05-243.59.50.png alt></p><h2 id=features-attention>Features->Attention</h2><p>分层注意力</p><p>之前是把3个feature maps各自生成的attention maps相加</p><p><a href=https://zhuanlan.zhihu.com/p/398546919>https://zhuanlan.zhihu.com/p/398546919</a></p><p>对于小目标，小尺度feature map无法提供必要的分辨率信息，所以还需结合大尺度的feature map。还有个原因是在深层图做下采样损失过多信息，小目标信息或许已经被忽略。（下采样）（我这边用上采样）因为下采样还是会丢失大量信息</p><p><a href=https://blog.csdn.net/weixin_41560402/article/details/109540128>https://blog.csdn.net/weixin_41560402/article/details/109540128</a></p><p><a href=https://zhuanlan.zhihu.com/p/266472242>https://zhuanlan.zhihu.com/p/266472242</a></p><p>或者用超分做上采样 用平均池化做下采样</p><p>现在</p><p>F1:512,28,28</p><p>F2:1024,14,14</p><p>F3:2048,7,7</p><p>A1:32,28,28</p><p>A2:32,14,14</p><p>A3:32,7,7</p><p>全都上采样成 32，28，28</p><p>把f1生成的a1，用于和f2生成a2，a2再用于生成f3。</p><p>512,28,28->32,28,28 下采样到 32,14,14 （a1）</p><p>f2 & a1 bap得到 -> PF1(512,28,28)</p><p>1024,14,14->32,14,14 下采样到 32,7,7 (a2)</p><p>F3 & a2 bap得到 -> PF2(1024,14,14)</p><p>2048,7,7->32,7,7 不用下采样(a3)</p><p>F1 接self-attention得到f1‘</p><p><img src=https://halfbit.oss-cn-hangzhou.aliyuncs.com/2022-05-243.56.42.png alt></p><h2 id=代码>代码</h2><p>Q:分别从哪里提取features（f1 f2 f3），features转为attention的方式（得到fig3中A的过程(把texture的和target的attention相加即可)）。看一下model.py吧。</p><p>A:[mark]</p><p>1.实际上是实现了f2，f3的提取，得到attention maps然后用f3和这个attention maps做bap（和wsdan一样）</p><p>2.网络返回的有3个东西。logits = self.fc(feature_matrix * 100)，layer3以后的feature1（1024，14，14），attention maps（训练的时候返回三张，预测时返回1张）</p><p>3.注释了的跳过，没用上的跳过。model看完感觉主要创新点也看完了啊。</p><p><a href=https://blog.csdn.net/gdymind/article/details/82388068>https://blog.csdn.net/gdymind/article/details/82388068</a></p><p>4.J. Fu, H. Zheng, and T. Mei, “Look closer to see better: Recurrent attention convolutional neural network for ﬁne-grained image recognition,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2017, pp. 4438–4446. 引用找这篇。</p><h2 id=过拟合>过拟合</h2><p>1.加了dropout，not l2norm</p><p>2.修改epoch，10-40</p><p>3.原图crop。</p><h2 id=test>Test</h2></div><footer class=post-footer><nav class=post-nav><a class=prev href=/post/pooling/><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417l277.93508-310.326815c11.338233-12.190647 11.035334-32.285311-.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"/></svg></i><span class="prev-text nav-default">Bilinear Pooling & loss</span>
<span class="prev-text nav-mobile">上一篇</span></a>
<a class=next href=/post/luna%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/><span class="next-text nav-default">luna论文笔记</span>
<span class="prev-text nav-mobile">下一篇</span>
<i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697c-11.777231-11.500939-30.216186-10.304694-41.178865 2.712784z"/></svg></i></a></nav></footer></article></div></div></main><footer id=footer class=footer><div class=icon-links><a href=http://odyssey.halfbit.top/index.xml rel="noopener alternate" type=application/rss+xml class=iconfont title=rss target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="30" height="30"><path d="M819.157333 1024C819.157333 574.592 449.408 204.8.0 204.8V0c561.706667.0 1024 462.293333 1024 1024H819.157333zM140.416 743.04a140.8 140.8.0 01140.501333 140.586667A140.928 140.928.0 01140.074667 1024C62.72 1024 0 961.109333.0 883.626667S62.933333 743.082667 140.416 743.04zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667v-199.04c372.352.0 678.784 306.517333 678.784 678.826667z"/></svg></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme - <a class=theme-link href=https://github.com/xianmin/hugo-theme-jane>Jane</a></span>
<span class=copyright-year>&copy;
2022
<span class=heart><i class=iconfont><svg class="icon" viewBox="0 0 1025 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="14" height="14"><path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7.0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1.0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2.0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3.1-42.5-8-83.6-24-122.2z" fill="#8a8a8a"/></svg></i></span><span class=author>momoka</span></span></div></footer><div class=back-to-top id=back-to-top><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="35" height="35"><path d="M510.866688 227.694839 95.449397 629.218702h235.761562L329.15309 958.01517h362.40389L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777h894.052392v131.813095H63.840492V63.962777zm0 0"/></svg></i></div></div><script type=text/javascript src=/lib/jquery/jquery-3.2.1.min.js></script>
<script type=text/javascript src=/lib/slideout/slideout-1.0.1.min.js></script>
<script type=text/javascript src=/js/main.638251f4230630f0335d8c6748e53a96f94b72670920b60c09a56fdc8bece214.js integrity="sha256-Y4JR9CMGMPAzXYxnSOU6lvlLcmcJILYMCaVv3Ivs4hQ=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/load-photoswipe.js></script>
<script type=text/javascript src=/lib/photoswipe/photoswipe.min.js></script>
<script type=text/javascript src=/lib/photoswipe/photoswipe-ui-default.min.js></script></body></html>