<!doctype html><html lang=zh-cn itemscope itemtype=http://schema.org/WebPage><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>luna论文笔记 - ODYSSEY</title><meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,user-scalable=yes"><meta name=MobileOptimized content="width"><meta name=HandheldFriendly content="true"><meta name=applicable-device content="pc,mobile"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=mobile-web-app-capable content="yes"><meta name=author content="momo"><meta name=description content="LUNA: Localizing Unfamiliarity Near Acquaintance for Open-set Long-Tailed Recognition https://www.aaai.org/AAAI22Papers/AAAI-10200.CaiJ.pdf 2022 AAAI 博客 center loss： https://blog.csdn.net/duan19920101/article/details/104445423 https://github.com/KaiyangZhou/pytorch-center-loss （center_loss.py） 动机 However, the performances of the state-of-the-art object recognition methods mostly bias on the sample-rich classes that have been seen in the training set, with a limited ability on classifying the"><meta name=generator content="Hugo 0.102.3"><link rel=canonical href=http://odyssey.halfbit.top/post/luna%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/><link rel=icon href=/favicon.ico><link rel=stylesheet href=/sass/jane.min.fa4b2b9f31b5c6d0b683db81157a9226e17b06e61911791ab547242a4a0556f2.css integrity="sha256-+ksrnzG1xtC2g9uBFXqSJuF7BuYZEXkatUckKkoFVvI=" media=screen crossorigin=anonymous><meta property="og:title" content="luna论文笔记"><meta property="og:description" content="LUNA: Localizing Unfamiliarity Near Acquaintance for Open-set Long-Tailed Recognition https://www.aaai.org/AAAI22Papers/AAAI-10200.CaiJ.pdf 2022 AAAI 博客 center loss： https://blog.csdn.net/duan19920101/article/details/104445423 https://github.com/KaiyangZhou/pytorch-center-loss （center_loss.py） 动机 However, the performances of the state-of-the-art object recognition methods mostly bias on the sample-rich classes that have been seen in the training set, with a limited ability on classifying the"><meta property="og:type" content="article"><meta property="og:url" content="http://odyssey.halfbit.top/post/luna%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-05-23T00:00:00+00:00"><meta property="article:modified_time" content="2022-05-23T00:00:00+00:00"><meta itemprop=name content="luna论文笔记"><meta itemprop=description content="LUNA: Localizing Unfamiliarity Near Acquaintance for Open-set Long-Tailed Recognition https://www.aaai.org/AAAI22Papers/AAAI-10200.CaiJ.pdf 2022 AAAI 博客 center loss： https://blog.csdn.net/duan19920101/article/details/104445423 https://github.com/KaiyangZhou/pytorch-center-loss （center_loss.py） 动机 However, the performances of the state-of-the-art object recognition methods mostly bias on the sample-rich classes that have been seen in the training set, with a limited ability on classifying the"><meta itemprop=datePublished content="2022-05-23T00:00:00+00:00"><meta itemprop=dateModified content="2022-05-23T00:00:00+00:00"><meta itemprop=wordCount content="1844"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="luna论文笔记"><meta name=twitter:description content="LUNA: Localizing Unfamiliarity Near Acquaintance for Open-set Long-Tailed Recognition https://www.aaai.org/AAAI22Papers/AAAI-10200.CaiJ.pdf 2022 AAAI 博客 center loss： https://blog.csdn.net/duan19920101/article/details/104445423 https://github.com/KaiyangZhou/pytorch-center-loss （center_loss.py） 动机 However, the performances of the state-of-the-art object recognition methods mostly bias on the sample-rich classes that have been seen in the training set, with a limited ability on classifying the"><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>ODYSSEY</a></div><div class=mobile-navbar-icon><span></span>
<span></span>
<span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><li class=mobile-menu-item><a class=menu-item-link href=http://odyssey.halfbit.top/>Home</a></li><li class=mobile-menu-item><a class=menu-item-link href=http://odyssey.halfbit.top/post/>Archives</a></li><li class=mobile-menu-item><a class=menu-item-link href=http://odyssey.halfbit.top/categories/>Categories</a></li><li class=mobile-menu-item><a class=menu-item-link href=http://odyssey.halfbit.top/about/>About</a></li></ul></nav><link rel=stylesheet href=/lib/photoswipe/photoswipe.min.css><link rel=stylesheet href=/lib/photoswipe/default-skin/default-skin.min.css><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><header id=header class="header container"><div class=logo-wrapper><a href=/ class=logo>ODYSSEY</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=http://odyssey.halfbit.top/>Home</a></li><li class=menu-item><a class=menu-item-link href=http://odyssey.halfbit.top/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=http://odyssey.halfbit.top/categories/>Categories</a></li><li class=menu-item><a class=menu-item-link href=http://odyssey.halfbit.top/about/>About</a></li></ul></nav></header><div id=mobile-panel><main id=main class="main bg-llight"><div class=content-wrapper><div id=content class="content container"><article class="post bg-white"><header class=post-header><h1 class=post-title>luna论文笔记</h1><div class=post-meta><time datetime=2022-05-23 class=post-time>2022-05-23</time><div class=post-category><a href=http://odyssey.halfbit.top/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/>论文笔记</a>
<a href=http://odyssey.halfbit.top/categories/aaai/>AAAI</a></div></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>文章目录</h2><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#博客>博客</a></li><li><a href=#动机>动机</a></li><li><a href=#related-work>related work</a></li><li><a href=#attention-regularization-loss>attention regularization loss</a></li><li><a href=#center-loss>center loss</a></li><li><a href=#wcenter-loss加权的center-loss>wcenter loss（加权的center loss）</a></li></ul></nav></div></div><div class=post-content><h1 id=luna-localizing-unfamiliarity-near-acquaintance-for-open-set-long-tailed-recognition>LUNA: Localizing Unfamiliarity Near Acquaintance for Open-set Long-Tailed Recognition</h1><p><a href=https://www.aaai.org/AAAI22Papers/AAAI-10200.CaiJ.pdf>https://www.aaai.org/AAAI22Papers/AAAI-10200.CaiJ.pdf</a></p><p>2022 AAAI</p><h2 id=博客>博客</h2><p>center loss：</p><p><a href=https://blog.csdn.net/duan19920101/article/details/104445423>https://blog.csdn.net/duan19920101/article/details/104445423</a></p><p><a href=https://github.com/KaiyangZhou/pytorch-center-loss>https://github.com/KaiyangZhou/pytorch-center-loss</a> （center_loss.py）</p><h2 id=动机>动机</h2><p>However, the performances of the state-of-the-art object recognition methods mostly bias on the sample-rich classes that have been seen in the training set, with a limited ability on classifying the sample-few classes, not to mention the new/novel classes of objects (Kang et al. 2019; Zhou et al. 2020).分类表现倾向于那些样本丰富的类别，对于那些样本量比较少的类别，分类能力有限。两个问题：open-set和长尾。两个挑战往往是重合的。Open-set Long-Tailed Recognition简称OLTR。</p><p>作者提出一个度量学习框架，称为Localizing Unfamiliarity Near Acquaintance（LUNA），根据深度CNN特征的局部密度（local density）来定量测量开放集长尾识别任务的新颖程度。通过LUNA，可以精确地回答两个问题：（1）输入是否是新的；（2）如果不是，是哪一类；if yes, what is the unfamiliarity level of the new class concerning the pretrained acquaintance classes. 如果是新类，这个新类和预训练好的类别的不相似等级是多少。综上所述，我们声称我们的贡献和技术革新如下。</p><p>1、我们收集了一个新的注释良好的真实海洋物种开放长尾（MS-LT）数据集。作为细粒度领域的第一个自然OLTR数据集，它将是对现有人工重新采样的OLTR数据集的有力补充。它对表征学习和新物种检测提出了新的挑战。</p><p>2、为了使类别单独集中在特征空间中，特征提取器通过新提出的损失，即加权中心损失**（wcenter-loss）**来训练，以最小化它们的类内距离，从而在高维空间中形成密集的聚类。它集中了头类的深层特征，同时保留了尾类的分类精度，从而使特征更加鲜明。</p><p>3、为了衡量新类的不熟悉程度，评价与熟人类的接近程度，我们提出了一个LUNA因子，一个基于深层特征的相对密度的离群指标，它对分布是自适应的。LUNA因子是第一个对长尾分布下的新颖性进行定量测量的指标。</p><p>4、我们在MS-LT数据集和两个常用的人工采样数据集ImageNet-LT和Place-LT上对LUNA进行了广泛的评估，包括长尾和Openet识别。结果表明，LUNA在封闭集上明显优于最先进的方法4-6%，在开放集设置下，F-measure平均提高4%。</p><h2 id=related-work>related work</h2><p>1.OLTR</p><p>2.Novelty Detection</p><p>3.深度度量学习（DML）。DML是在高维嵌入空间中最大化类间距离和最小化类内距离。两种类型的DML方法被广泛使用：a）用类级标签学习；b）图像级标签。前者从分类模型中获得嵌入，例如ArcFace（Deng等人，2019），CosFace（Wang等人，2018）。后者通过损失函数直接优化采样图像对或组的嵌入距离，不产生DML后的分类结果，如对比性（Chopra, Hadsell, and LeCun 2005）、三联体（Schroff, Kalenichenko, and Philbin 2015）和中心（Wen et al. 2016）损失。这些DML算法都是在训练数据充足且普遍平衡的假设下进行的，这对于长尾设置来说并不成立。对于类级的DML，分类精度主要受数据分布偏差的影响；而对于图像级的DML，少数照片的类容易被过度拟合。本文提出了一个频率感知的损失函数来同时解决数据不平衡和度量学习问题。</p><h2 id=attention-regularization-loss>attention regularization loss</h2><p>f和c各自做L2_norm。作差，平方，sum，mean。</p><p><img src=https://halfbit.oss-cn-hangzhou.aliyuncs.com/2022-05-214.37.10.png alt></p><p>官方实现。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>def calculate_pooling_center_loss(features, label, alfa, nrof_classes, weights, name):
</span></span><span class=line><span class=cl>    features = tf.reshape(features, [features.shape[0], -1])
</span></span><span class=line><span class=cl>    label = tf.argmax(label, 1)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    nrof_features = features.get_shape()[1]
</span></span><span class=line><span class=cl>    centers = tf.get_variable(name, [nrof_classes, nrof_features], dtype=tf.float32,
</span></span><span class=line><span class=cl>                              initializer=tf.constant_initializer(0), trainable=False)
</span></span><span class=line><span class=cl>    label = tf.reshape(label, [-1])
</span></span><span class=line><span class=cl>    centers_batch = tf.gather(centers, label)
</span></span><span class=line><span class=cl>    centers_batch = tf.nn.l2_normalize(centers_batch, axis=-1)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    diff = (1 - alfa) * (centers_batch - features)
</span></span><span class=line><span class=cl>    centers = tf.scatter_sub(centers, label, diff)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    with tf.control_dependencies([centers]):
</span></span><span class=line><span class=cl>        distance = tf.square(features - centers_batch)
</span></span><span class=line><span class=cl>        distance = tf.reduce_sum(distance, axis=-1)
</span></span><span class=line><span class=cl>        center_loss = tf.reduce_mean(distance)
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    center_loss = tf.identity(center_loss * weights, name=name + &#39;_loss&#39;)
</span></span><span class=line><span class=cl>    return center_loss
</span></span></code></pre></td></tr></table></div></div><p>训练集里面，一个类别中的样本越少，它的权重就越高。取样本数量的倒数？作为lambda加进去？（感觉太粗暴了，但是应该可行吧）</p><h2 id=center-loss>center loss</h2><p>Cyi表示第yi个类别的特征中心，Xi表示全连接层之前的特征。实际使用的时候，m表示mini-batch的大小。因此这个公式就是希望一个batch中的每个样本的feature离feature 的中心的距离的平方和要越小越好，也就是类内距离要越小越好。</p><p><img src=https://halfbit.oss-cn-hangzhou.aliyuncs.com/2022-05-237.49.52.png alt></p><p><img src=https://halfbit.oss-cn-hangzhou.aliyuncs.com/2022-05-237.26.03.png alt></p><p>目标函数如下：</p><p><img src=https://halfbit.oss-cn-hangzhou.aliyuncs.com/2022-05-237.34.53.png alt></p><h2 id=wcenter-loss加权的center-loss>wcenter loss（加权的center loss）</h2><p><img src=https://halfbit.oss-cn-hangzhou.aliyuncs.com/2022-05-237.35.53.png alt></p><p>在长尾数据集的情况下，由于样本数量少得多，尾部类的分布往往比较稀疏，更容易与特征空间中的其他聚类混在一起。因此，我们提出了一个加权中心（wcenter）损失，以适应不平衡的分布。</p><p>c是类别的索引。λi 是 yi所属的类j的归一化频率的权重。（黑人问号脸）</p><p>nj表示训练集中j类样本的数量。</p><p>基本上，λi是由最大频率值缩放的反转分布，表示为nj波浪，然后在[0，1]之间归一化。</p><p>一个类别中的样本越少，它的权重就越高。</p><p>请注意，λi应大于1，并处于交叉熵损失的同一尺度，以确保网络的收敛性。</p><p><img src=https://halfbit.oss-cn-hangzhou.aliyuncs.com/2022-05-237.26.10.png alt></p><p>目标函数改成下图，请注意，参数λ被嵌入到Lwc中，并为不同的类定制，以最小化类内距离，特别是尾部。</p><p><img src=https://halfbit.oss-cn-hangzhou.aliyuncs.com/2022-05-237.33.57.png alt></p></div><footer class=post-footer><nav class=post-nav><a class=prev href=/post/mag-sd%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417l277.93508-310.326815c11.338233-12.190647 11.035334-32.285311-.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"/></svg></i><span class="prev-text nav-default">MAG-SD论文笔记</span>
<span class="prev-text nav-mobile">上一篇</span></a>
<a class=next href=/post/ws-dan%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/><span class="next-text nav-default">WS-DAN论文笔记</span>
<span class="prev-text nav-mobile">下一篇</span>
<i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="18" height="18"><path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697c-11.777231-11.500939-30.216186-10.304694-41.178865 2.712784z"/></svg></i></a></nav></footer></article></div></div></main><footer id=footer class=footer><div class=icon-links><a href=http://odyssey.halfbit.top/index.xml rel="noopener alternate" type=application/rss+xml class=iconfont title=rss target=_blank><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="30" height="30"><path d="M819.157333 1024C819.157333 574.592 449.408 204.8.0 204.8V0c561.706667.0 1024 462.293333 1024 1024H819.157333zM140.416 743.04a140.8 140.8.0 01140.501333 140.586667A140.928 140.928.0 01140.074667 1024C62.72 1024 0 961.109333.0 883.626667S62.933333 743.082667 140.416 743.04zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667v-199.04c372.352.0 678.784 306.517333 678.784 678.826667z"/></svg></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a></span>
<span class=division>|</span>
<span class=theme-info>Theme - <a class=theme-link href=https://github.com/xianmin/hugo-theme-jane>Jane</a></span>
<span class=copyright-year>&copy;
2022
<span class=heart><i class=iconfont><svg class="icon" viewBox="0 0 1025 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="14" height="14"><path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7.0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1.0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2.0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3.1-42.5-8-83.6-24-122.2z" fill="#8a8a8a"/></svg></i></span><span class=author>momoka</span></span></div></footer><div class=back-to-top id=back-to-top><i class=iconfont><svg class="icon" viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="35" height="35"><path d="M510.866688 227.694839 95.449397 629.218702h235.761562L329.15309 958.01517h362.40389L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777h894.052392v131.813095H63.840492V63.962777zm0 0"/></svg></i></div></div><script type=text/javascript src=/lib/jquery/jquery-3.2.1.min.js></script>
<script type=text/javascript src=/lib/slideout/slideout-1.0.1.min.js></script>
<script type=text/javascript src=/js/main.638251f4230630f0335d8c6748e53a96f94b72670920b60c09a56fdc8bece214.js integrity="sha256-Y4JR9CMGMPAzXYxnSOU6lvlLcmcJILYMCaVv3Ivs4hQ=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/load-photoswipe.js></script>
<script type=text/javascript src=/lib/photoswipe/photoswipe.min.js></script>
<script type=text/javascript src=/lib/photoswipe/photoswipe-ui-default.min.js></script></body></html>